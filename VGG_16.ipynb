{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    " \n",
    "from tensorflow_vgg import vgg16\n",
    "from tensorflow_vgg import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'flower_photos/'\n",
    "contents = os.listdir(data_dir)\n",
    "classes = [each for each in contents if os.path.isdir(data_dir + each)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HanY\\Desktop\\VGG_16\\tensorflow_vgg\\vgg16.npy\n",
      "npy file loaded\n",
      "build model started\n",
      "build model finished: 1s\n",
      "Starting ben images\n",
      "20 images processed\n",
      "40 images processed\n",
      "60 images processed\n",
      "80 images processed\n",
      "100 images processed\n",
      "120 images processed\n",
      "140 images processed\n",
      "160 images processed\n",
      "180 images processed\n",
      "200 images processed\n",
      "220 images processed\n",
      "240 images processed\n",
      "260 images processed\n",
      "280 images processed\n",
      "300 images processed\n",
      "320 images processed\n",
      "340 images processed\n",
      "360 images processed\n",
      "380 images processed\n",
      "400 images processed\n",
      "420 images processed\n",
      "440 images processed\n",
      "460 images processed\n",
      "480 images processed\n",
      "500 images processed\n",
      "520 images processed\n",
      "540 images processed\n",
      "560 images processed\n",
      "580 images processed\n",
      "600 images processed\n",
      "620 images processed\n",
      "640 images processed\n",
      "660 images processed\n",
      "680 images processed\n",
      "700 images processed\n",
      "720 images processed\n",
      "740 images processed\n",
      "760 images processed\n",
      "780 images processed\n",
      "800 images processed\n",
      "820 images processed\n",
      "840 images processed\n",
      "860 images processed\n",
      "880 images processed\n",
      "900 images processed\n",
      "920 images processed\n",
      "940 images processed\n",
      "960 images processed\n",
      "980 images processed\n",
      "1000 images processed\n",
      "1020 images processed\n",
      "1040 images processed\n",
      "1060 images processed\n",
      "1080 images processed\n",
      "1100 images processed\n",
      "1120 images processed\n",
      "1140 images processed\n",
      "1160 images processed\n",
      "1180 images processed\n",
      "1200 images processed\n",
      "1220 images processed\n",
      "1240 images processed\n",
      "1260 images processed\n",
      "1280 images processed\n",
      "1300 images processed\n",
      "1320 images processed\n",
      "1340 images processed\n",
      "1360 images processed\n",
      "1380 images processed\n",
      "1400 images processed\n",
      "1420 images processed\n",
      "1440 images processed\n",
      "1460 images processed\n",
      "1480 images processed\n",
      "1500 images processed\n",
      "1520 images processed\n",
      "1540 images processed\n",
      "1560 images processed\n",
      "1580 images processed\n",
      "1600 images processed\n",
      "1620 images processed\n",
      "1640 images processed\n",
      "1660 images processed\n",
      "1680 images processed\n",
      "1700 images processed\n",
      "1720 images processed\n",
      "1740 images processed\n",
      "1760 images processed\n",
      "1780 images processed\n",
      "1800 images processed\n",
      "1820 images processed\n",
      "1840 images processed\n",
      "1860 images processed\n",
      "1880 images processed\n",
      "1900 images processed\n",
      "1920 images processed\n",
      "1940 images processed\n",
      "1960 images processed\n",
      "1980 images processed\n",
      "2000 images processed\n",
      "2020 images processed\n",
      "2040 images processed\n",
      "2060 images processed\n",
      "2077 images processed\n",
      "Starting mal images\n",
      "20 images processed\n",
      "40 images processed\n",
      "60 images processed\n",
      "80 images processed\n",
      "100 images processed\n",
      "120 images processed\n",
      "140 images processed\n",
      "160 images processed\n",
      "180 images processed\n",
      "200 images processed\n",
      "220 images processed\n",
      "240 images processed\n",
      "260 images processed\n",
      "280 images processed\n",
      "300 images processed\n",
      "320 images processed\n",
      "340 images processed\n",
      "360 images processed\n",
      "380 images processed\n",
      "400 images processed\n",
      "420 images processed\n",
      "440 images processed\n",
      "460 images processed\n",
      "480 images processed\n",
      "500 images processed\n",
      "520 images processed\n",
      "540 images processed\n",
      "560 images processed\n",
      "580 images processed\n",
      "600 images processed\n",
      "620 images processed\n",
      "640 images processed\n",
      "660 images processed\n",
      "680 images processed\n",
      "700 images processed\n",
      "720 images processed\n",
      "740 images processed\n",
      "760 images processed\n",
      "780 images processed\n",
      "800 images processed\n",
      "820 images processed\n",
      "840 images processed\n",
      "860 images processed\n",
      "880 images processed\n",
      "900 images processed\n",
      "920 images processed\n",
      "940 images processed\n",
      "960 images processed\n",
      "980 images processed\n",
      "1000 images processed\n",
      "1020 images processed\n",
      "1040 images processed\n",
      "1060 images processed\n",
      "1080 images processed\n",
      "1100 images processed\n",
      "1120 images processed\n",
      "1140 images processed\n",
      "1160 images processed\n",
      "1180 images processed\n",
      "1200 images processed\n",
      "1220 images processed\n",
      "1240 images processed\n",
      "1260 images processed\n",
      "1280 images processed\n",
      "1300 images processed\n",
      "1320 images processed\n",
      "1340 images processed\n",
      "1360 images processed\n",
      "1380 images processed\n",
      "1400 images processed\n",
      "1420 images processed\n",
      "1440 images processed\n",
      "1460 images processed\n",
      "1480 images processed\n",
      "1500 images processed\n",
      "1520 images processed\n",
      "1540 images processed\n",
      "1560 images processed\n",
      "1580 images processed\n",
      "1600 images processed\n",
      "1620 images processed\n",
      "1640 images processed\n",
      "1660 images processed\n",
      "1680 images processed\n",
      "1700 images processed\n",
      "1720 images processed\n",
      "1740 images processed\n",
      "1760 images processed\n",
      "1780 images processed\n",
      "1800 images processed\n",
      "1820 images processed\n",
      "1840 images processed\n",
      "1860 images processed\n",
      "1880 images processed\n",
      "1900 images processed\n",
      "1920 images processed\n",
      "1940 images processed\n",
      "1960 images processed\n",
      "1980 images processed\n",
      "2000 images processed\n",
      "2020 images processed\n",
      "2040 images processed\n",
      "2060 images processed\n",
      "2080 images processed\n",
      "2100 images processed\n",
      "2120 images processed\n",
      "2140 images processed\n",
      "2160 images processed\n",
      "2180 images processed\n",
      "2200 images processed\n",
      "2220 images processed\n",
      "2240 images processed\n",
      "2260 images processed\n",
      "2280 images processed\n",
      "2300 images processed\n",
      "2320 images processed\n",
      "2340 images processed\n",
      "2360 images processed\n",
      "2380 images processed\n",
      "2400 images processed\n",
      "2420 images processed\n",
      "2440 images processed\n",
      "2460 images processed\n",
      "2480 images processed\n",
      "2500 images processed\n",
      "2520 images processed\n",
      "2540 images processed\n",
      "2560 images processed\n",
      "2580 images processed\n",
      "2600 images processed\n",
      "2620 images processed\n",
      "2640 images processed\n",
      "2660 images processed\n",
      "2680 images processed\n",
      "2700 images processed\n",
      "2720 images processed\n",
      "2740 images processed\n",
      "2760 images processed\n",
      "2780 images processed\n",
      "2800 images processed\n",
      "2820 images processed\n",
      "2840 images processed\n",
      "2860 images processed\n",
      "2880 images processed\n",
      "2900 images processed\n",
      "2920 images processed\n",
      "2940 images processed\n",
      "2960 images processed\n",
      "2980 images processed\n",
      "3000 images processed\n",
      "3020 images processed\n",
      "3040 images processed\n",
      "3060 images processed\n",
      "3080 images processed\n",
      "3100 images processed\n",
      "3120 images processed\n",
      "3140 images processed\n",
      "3160 images processed\n",
      "3180 images processed\n",
      "3200 images processed\n",
      "3220 images processed\n",
      "3240 images processed\n",
      "3260 images processed\n",
      "3280 images processed\n",
      "3300 images processed\n",
      "3320 images processed\n",
      "3340 images processed\n",
      "3360 images processed\n",
      "3380 images processed\n",
      "3400 images processed\n",
      "3420 images processed\n",
      "3440 images processed\n",
      "3460 images processed\n",
      "3480 images processed\n",
      "3500 images processed\n",
      "3520 images processed\n",
      "3540 images processed\n",
      "3560 images processed\n",
      "3580 images processed\n",
      "3600 images processed\n",
      "3620 images processed\n",
      "3640 images processed\n",
      "3660 images processed\n",
      "3680 images processed\n",
      "3700 images processed\n",
      "3720 images processed\n",
      "3740 images processed\n",
      "3760 images processed\n",
      "3780 images processed\n",
      "3800 images processed\n",
      "3820 images processed\n",
      "3840 images processed\n",
      "3860 images processed\n",
      "3880 images processed\n",
      "3900 images processed\n",
      "3920 images processed\n",
      "3940 images processed\n",
      "3960 images processed\n",
      "3980 images processed\n",
      "4000 images processed\n",
      "4020 images processed\n",
      "4040 images processed\n",
      "4060 images processed\n",
      "4080 images processed\n",
      "4100 images processed\n",
      "4120 images processed\n",
      "4140 images processed\n",
      "4160 images processed\n",
      "4180 images processed\n",
      "4200 images processed\n",
      "4220 images processed\n",
      "4240 images processed\n",
      "4260 images processed\n"
     ]
    }
   ],
   "source": [
    "# 首先设置计算batch的值，如果运算平台的内存越大，这个值可以设置得越高\n",
    "batch_size = 20\n",
    "# 用codes_list来存储特征值\n",
    "codes_list = []\n",
    "# 用labels来存储花的类别\n",
    "labels = []\n",
    "# batch数组用来临时存储图片数据\n",
    "batch = []\n",
    " \n",
    "codes = None\n",
    " \n",
    "with tf.Session() as sess:\n",
    "    # 构建VGG16模型对象\n",
    "    vgg = vgg16.Vgg16()\n",
    "    input_ = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "    with tf.name_scope(\"content_vgg\"):\n",
    "        # 载入VGG16模型\n",
    "        vgg.build(input_)\n",
    "    \n",
    "    # 对每个不同种类的花分别用VGG16计算特征值\n",
    "    for each in classes:\n",
    "        print(\"Starting {} images\".format(each))\n",
    "        class_path = data_dir + each\n",
    "        files = os.listdir(class_path)\n",
    "        for ii, file in enumerate(files, 1):\n",
    "            # 载入图片并放入batch数组中\n",
    "            img = utils.load_image(os.path.join(class_path, file))\n",
    "            batch.append(img.reshape((1, 224, 224, 3)))\n",
    "            labels.append(each)\n",
    "            \n",
    "            # 如果图片数量到了batch_size则开始具体的运算\n",
    "            if ii % batch_size == 0 or ii == len(files):\n",
    "                images = np.concatenate(batch)\n",
    " \n",
    "                feed_dict = {input_: images}\n",
    "                # 计算特征值\n",
    "                codes_batch = sess.run(vgg.relu6, feed_dict=feed_dict)\n",
    "                \n",
    "                # 将结果放入到codes数组中\n",
    "                if codes is None:\n",
    "                    codes = codes_batch\n",
    "                else:\n",
    "                    codes = np.concatenate((codes, codes_batch))\n",
    "                \n",
    "                # 清空数组准备下一个batch的计算\n",
    "                batch = []\n",
    "                print('{} images processed'.format(ii))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('codes', 'w') as f:\n",
    "    codes.tofile(f)\n",
    "    \n",
    "import csv\n",
    "with open('labels', 'w') as f:\n",
    "    writer = csv.writer(f, delimiter='\\n')\n",
    "    writer.writerow(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HanY\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# data = ['北京', '上海']\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoded = label_encoder.fit_transform(labels)\n",
    "# print(label_encoded)\n",
    "labels_vecs = OneHotEncoder()\n",
    "labels_vecs = labels_vecs.fit_transform(label_encoded.reshape(-1, 1)).toarray()\n",
    "print(labels_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6337, 4096)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shapes (x, y): (5069, 4096) (5069, 2)\n",
      "Validation shapes (x, y): (634, 4096) (634, 2)\n",
      "Test shapes (x, y): (634, 4096) (634, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    " \n",
    "ss = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\n",
    " \n",
    "train_idx, val_idx = next(ss.split(codes, labels))\n",
    " \n",
    "half_val_len = int(len(val_idx)/2)\n",
    "val_idx, test_idx = val_idx[:half_val_len], val_idx[half_val_len:]\n",
    " \n",
    "train_x, train_y = codes[train_idx], labels_vecs[train_idx]\n",
    "val_x, val_y = codes[val_idx], labels_vecs[val_idx]\n",
    "test_x, test_y = codes[test_idx], labels_vecs[test_idx]\n",
    " \n",
    "print(\"Train shapes (x, y):\", train_x.shape, train_y.shape)\n",
    "print(\"Validation shapes (x, y):\", val_x.shape, val_y.shape)\n",
    "print(\"Test shapes (x, y):\", test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\HanY\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-10-68db56c074cb>:13: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\HanY\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\metrics_impl.py:526: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\HanY\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\metrics_impl.py:788: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From C:\\Users\\HanY\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\confusion_matrix.py:193: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\HanY\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\confusion_matrix.py:194: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# 输入数据的维度\n",
    "inputs_ = tf.placeholder(tf.float32, shape=[None, codes.shape[1]])\n",
    "# 标签数据的维度\n",
    "labels_ = tf.placeholder(tf.int64, shape=[None, labels_vecs.shape[1]])\n",
    " \n",
    "# 加入一个256维的全连接的层\n",
    "fc = tf.contrib.layers.fully_connected(inputs_, 256)\n",
    " \n",
    "# 加入一个5维的全连接层\n",
    "logits = tf.contrib.layers.fully_connected(fc, labels_vecs.shape[1], activation_fn=None)\n",
    " \n",
    "# 计算cross entropy值\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=labels_, logits=logits)\n",
    " \n",
    "# 计算损失函数\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    " \n",
    "# 采用用得最广泛的AdamOptimizer优化器\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    " \n",
    "# 得到最后的预测分布\n",
    "predicted = tf.nn.softmax(logits)\n",
    " \n",
    "# 计算准确度\n",
    "correct_pred = tf.equal(tf.argmax(predicted, 1), tf.argmax(labels_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "auc_value, auc_op = tf.metrics.auc(labels_, predicted)\n",
    "confusion_matrix = tf.confusion_matrix(tf.argmax(labels_, 1), tf.argmax(predicted, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_3:0\", shape=(?, 2), dtype=int64)\n",
      "Tensor(\"Softmax:0\", shape=(?, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print (labels_)\n",
    "print (predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(x, y, n_batches=10):\n",
    "    \"\"\" 这是一个生成器函数，按照n_batches的大小将数据划分了小块 \"\"\"\n",
    "    batch_size = len(x)//n_batches\n",
    "    \n",
    "    for ii in range(0, n_batches*batch_size, batch_size):\n",
    "        # 如果不是最后一个batch，那么这个batch中应该有batch_size个数据\n",
    "        if ii != (n_batches-1)*batch_size:\n",
    "            X, Y = x[ii: ii+batch_size], y[ii: ii+batch_size] \n",
    "        # 否则的话，那剩余的不够batch_size的数据都凑入到一个batch中\n",
    "        else:\n",
    "            X, Y = x[ii:], y[ii:]\n",
    "        # 生成器语法，返回X和Y\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40 Iteration: 0 Training loss: 3.61856\n",
      "Epoch: 1/40 Iteration: 1 Training loss: 23.95444\n",
      "Epoch: 1/40 Iteration: 2 Training loss: 21.41974\n",
      "Epoch: 1/40 Iteration: 3 Training loss: 12.44093\n",
      "Epoch: 1/40 Iteration: 4 Training loss: 6.84237\n",
      "Epoch: 0/40 Iteration: 5 Validation Acc: 0.6483\n",
      "Epoch: 1/40 Iteration: 5 Training loss: 3.46621\n",
      "Epoch: 1/40 Iteration: 6 Training loss: 1.17880\n",
      "Epoch: 1/40 Iteration: 7 Training loss: 0.62202\n",
      "Epoch: 1/40 Iteration: 8 Training loss: 0.69559\n",
      "Epoch: 1/40 Iteration: 9 Training loss: 0.72284\n",
      "Epoch: 0/40 Iteration: 10 Validation Acc: 0.3612\n",
      "Epoch: 2/40 Iteration: 10 Training loss: 0.73161\n",
      "Epoch: 2/40 Iteration: 11 Training loss: 0.71140\n",
      "Epoch: 2/40 Iteration: 12 Training loss: 0.71059\n",
      "Epoch: 2/40 Iteration: 13 Training loss: 0.69921\n",
      "Epoch: 2/40 Iteration: 14 Training loss: 0.69676\n",
      "Epoch: 1/40 Iteration: 15 Validation Acc: 0.3517\n",
      "Epoch: 2/40 Iteration: 15 Training loss: 0.69639\n",
      "Epoch: 2/40 Iteration: 16 Training loss: 0.69390\n",
      "Epoch: 2/40 Iteration: 17 Training loss: 0.69433\n",
      "Epoch: 2/40 Iteration: 18 Training loss: 0.69293\n",
      "Epoch: 2/40 Iteration: 19 Training loss: 0.69325\n",
      "Epoch: 1/40 Iteration: 20 Validation Acc: 0.6404\n",
      "Epoch: 3/40 Iteration: 20 Training loss: 0.69320\n",
      "Epoch: 3/40 Iteration: 21 Training loss: 0.69259\n",
      "Epoch: 3/40 Iteration: 22 Training loss: 0.69250\n",
      "Epoch: 3/40 Iteration: 23 Training loss: 0.69177\n",
      "Epoch: 3/40 Iteration: 24 Training loss: 0.69248\n",
      "Epoch: 2/40 Iteration: 25 Validation Acc: 0.6435\n",
      "Epoch: 3/40 Iteration: 25 Training loss: 0.69136\n",
      "Epoch: 3/40 Iteration: 26 Training loss: 0.69175\n",
      "Epoch: 3/40 Iteration: 27 Training loss: 0.69205\n",
      "Epoch: 3/40 Iteration: 28 Training loss: 0.69139\n",
      "Epoch: 3/40 Iteration: 29 Training loss: 0.69110\n",
      "Epoch: 2/40 Iteration: 30 Validation Acc: 0.6435\n",
      "Epoch: 4/40 Iteration: 30 Training loss: 0.69126\n",
      "Epoch: 4/40 Iteration: 31 Training loss: 0.69052\n",
      "Epoch: 4/40 Iteration: 32 Training loss: 0.69057\n",
      "Epoch: 4/40 Iteration: 33 Training loss: 0.68954\n",
      "Epoch: 4/40 Iteration: 34 Training loss: 0.69000\n",
      "Epoch: 3/40 Iteration: 35 Validation Acc: 0.6435\n",
      "Epoch: 4/40 Iteration: 35 Training loss: 0.68899\n",
      "Epoch: 4/40 Iteration: 36 Training loss: 0.68931\n",
      "Epoch: 4/40 Iteration: 37 Training loss: 0.68927\n",
      "Epoch: 4/40 Iteration: 38 Training loss: 0.68913\n",
      "Epoch: 4/40 Iteration: 39 Training loss: 0.68813\n",
      "Epoch: 3/40 Iteration: 40 Validation Acc: 0.6435\n",
      "Epoch: 5/40 Iteration: 40 Training loss: 0.68906\n",
      "Epoch: 5/40 Iteration: 41 Training loss: 0.68780\n",
      "Epoch: 5/40 Iteration: 42 Training loss: 0.68837\n",
      "Epoch: 5/40 Iteration: 43 Training loss: 0.68693\n",
      "Epoch: 5/40 Iteration: 44 Training loss: 0.68723\n",
      "Epoch: 4/40 Iteration: 45 Validation Acc: 0.6451\n",
      "Epoch: 5/40 Iteration: 45 Training loss: 0.68605\n",
      "Epoch: 5/40 Iteration: 46 Training loss: 0.68639\n",
      "Epoch: 5/40 Iteration: 47 Training loss: 0.68631\n",
      "Epoch: 5/40 Iteration: 48 Training loss: 0.68669\n",
      "Epoch: 5/40 Iteration: 49 Training loss: 0.68487\n",
      "Epoch: 4/40 Iteration: 50 Validation Acc: 0.6467\n",
      "Epoch: 6/40 Iteration: 50 Training loss: 0.68668\n",
      "Epoch: 6/40 Iteration: 51 Training loss: 0.68519\n",
      "Epoch: 6/40 Iteration: 52 Training loss: 0.68610\n",
      "Epoch: 6/40 Iteration: 53 Training loss: 0.68417\n",
      "Epoch: 6/40 Iteration: 54 Training loss: 0.68397\n",
      "Epoch: 5/40 Iteration: 55 Validation Acc: 0.6483\n",
      "Epoch: 6/40 Iteration: 55 Training loss: 0.68286\n",
      "Epoch: 6/40 Iteration: 56 Training loss: 0.68325\n",
      "Epoch: 6/40 Iteration: 57 Training loss: 0.68349\n",
      "Epoch: 6/40 Iteration: 58 Training loss: 0.68403\n",
      "Epoch: 6/40 Iteration: 59 Training loss: 0.68151\n",
      "Epoch: 5/40 Iteration: 60 Validation Acc: 0.6498\n",
      "Epoch: 7/40 Iteration: 60 Training loss: 0.68409\n",
      "Epoch: 7/40 Iteration: 61 Training loss: 0.68244\n",
      "Epoch: 7/40 Iteration: 62 Training loss: 0.68368\n",
      "Epoch: 7/40 Iteration: 63 Training loss: 0.68133\n",
      "Epoch: 7/40 Iteration: 64 Training loss: 0.68004\n",
      "Epoch: 6/40 Iteration: 65 Validation Acc: 0.6467\n",
      "Epoch: 7/40 Iteration: 65 Training loss: 0.67919\n",
      "Epoch: 7/40 Iteration: 66 Training loss: 0.67993\n",
      "Epoch: 7/40 Iteration: 67 Training loss: 0.68065\n",
      "Epoch: 7/40 Iteration: 68 Training loss: 0.68134\n",
      "Epoch: 7/40 Iteration: 69 Training loss: 0.67789\n",
      "Epoch: 6/40 Iteration: 70 Validation Acc: 0.6514\n",
      "Epoch: 8/40 Iteration: 70 Training loss: 0.68169\n",
      "Epoch: 8/40 Iteration: 71 Training loss: 0.67953\n",
      "Epoch: 8/40 Iteration: 72 Training loss: 0.68154\n",
      "Epoch: 8/40 Iteration: 73 Training loss: 0.67795\n",
      "Epoch: 8/40 Iteration: 74 Training loss: 0.67654\n",
      "Epoch: 7/40 Iteration: 75 Validation Acc: 0.6530\n",
      "Epoch: 8/40 Iteration: 75 Training loss: 0.67465\n",
      "Epoch: 8/40 Iteration: 76 Training loss: 0.67686\n",
      "Epoch: 8/40 Iteration: 77 Training loss: 0.67760\n",
      "Epoch: 8/40 Iteration: 78 Training loss: 0.67872\n",
      "Epoch: 8/40 Iteration: 79 Training loss: 0.67451\n",
      "Epoch: 7/40 Iteration: 80 Validation Acc: 0.6562\n",
      "Epoch: 9/40 Iteration: 80 Training loss: 0.67867\n",
      "Epoch: 9/40 Iteration: 81 Training loss: 0.67608\n",
      "Epoch: 9/40 Iteration: 82 Training loss: 0.67873\n",
      "Epoch: 9/40 Iteration: 83 Training loss: 0.67473\n",
      "Epoch: 9/40 Iteration: 84 Training loss: 0.67317\n",
      "Epoch: 8/40 Iteration: 85 Validation Acc: 0.6562\n",
      "Epoch: 9/40 Iteration: 85 Training loss: 0.67112\n",
      "Epoch: 9/40 Iteration: 86 Training loss: 0.67373\n",
      "Epoch: 9/40 Iteration: 87 Training loss: 0.67427\n",
      "Epoch: 9/40 Iteration: 88 Training loss: 0.67630\n",
      "Epoch: 9/40 Iteration: 89 Training loss: 0.67113\n",
      "Epoch: 8/40 Iteration: 90 Validation Acc: 0.6562\n",
      "Epoch: 10/40 Iteration: 90 Training loss: 0.67583\n",
      "Epoch: 10/40 Iteration: 91 Training loss: 0.67249\n",
      "Epoch: 10/40 Iteration: 92 Training loss: 0.67642\n",
      "Epoch: 10/40 Iteration: 93 Training loss: 0.67137\n",
      "Epoch: 10/40 Iteration: 94 Training loss: 0.67006\n",
      "Epoch: 9/40 Iteration: 95 Validation Acc: 0.6577\n",
      "Epoch: 10/40 Iteration: 95 Training loss: 0.66669\n",
      "Epoch: 10/40 Iteration: 96 Training loss: 0.67092\n",
      "Epoch: 10/40 Iteration: 97 Training loss: 0.67134\n",
      "Epoch: 10/40 Iteration: 98 Training loss: 0.67411\n",
      "Epoch: 10/40 Iteration: 99 Training loss: 0.66799\n",
      "Epoch: 9/40 Iteration: 100 Validation Acc: 0.6577\n",
      "Epoch: 11/40 Iteration: 100 Training loss: 0.67323\n",
      "Epoch: 11/40 Iteration: 101 Training loss: 0.66898\n",
      "Epoch: 11/40 Iteration: 102 Training loss: 0.67419\n",
      "Epoch: 11/40 Iteration: 103 Training loss: 0.66822\n",
      "Epoch: 11/40 Iteration: 104 Training loss: 0.66708\n",
      "Epoch: 10/40 Iteration: 105 Validation Acc: 0.6593\n",
      "Epoch: 11/40 Iteration: 105 Training loss: 0.66310\n",
      "Epoch: 11/40 Iteration: 106 Training loss: 0.66808\n",
      "Epoch: 11/40 Iteration: 107 Training loss: 0.66835\n",
      "Epoch: 11/40 Iteration: 108 Training loss: 0.67171\n",
      "Epoch: 11/40 Iteration: 109 Training loss: 0.66457\n",
      "Epoch: 10/40 Iteration: 110 Validation Acc: 0.6593\n",
      "Epoch: 12/40 Iteration: 110 Training loss: 0.67048\n",
      "Epoch: 12/40 Iteration: 111 Training loss: 0.66580\n",
      "Epoch: 12/40 Iteration: 112 Training loss: 0.67184\n",
      "Epoch: 12/40 Iteration: 113 Training loss: 0.66521\n",
      "Epoch: 12/40 Iteration: 114 Training loss: 0.66412\n",
      "Epoch: 11/40 Iteration: 115 Validation Acc: 0.6593\n",
      "Epoch: 12/40 Iteration: 115 Training loss: 0.65980\n",
      "Epoch: 12/40 Iteration: 116 Training loss: 0.66542\n",
      "Epoch: 12/40 Iteration: 117 Training loss: 0.66561\n",
      "Epoch: 12/40 Iteration: 118 Training loss: 0.66943\n",
      "Epoch: 12/40 Iteration: 119 Training loss: 0.66138\n",
      "Epoch: 11/40 Iteration: 120 Validation Acc: 0.6577\n",
      "Epoch: 13/40 Iteration: 120 Training loss: 0.66800\n",
      "Epoch: 13/40 Iteration: 121 Training loss: 0.66255\n",
      "Epoch: 13/40 Iteration: 122 Training loss: 0.66959\n",
      "Epoch: 13/40 Iteration: 123 Training loss: 0.66236\n",
      "Epoch: 13/40 Iteration: 124 Training loss: 0.66119\n",
      "Epoch: 12/40 Iteration: 125 Validation Acc: 0.6593\n",
      "Epoch: 13/40 Iteration: 125 Training loss: 0.65675\n",
      "Epoch: 13/40 Iteration: 126 Training loss: 0.66289\n",
      "Epoch: 13/40 Iteration: 127 Training loss: 0.66291\n",
      "Epoch: 13/40 Iteration: 128 Training loss: 0.66706\n",
      "Epoch: 13/40 Iteration: 129 Training loss: 0.65814\n",
      "Epoch: 12/40 Iteration: 130 Validation Acc: 0.6625\n",
      "Epoch: 14/40 Iteration: 130 Training loss: 0.66539\n",
      "Epoch: 14/40 Iteration: 131 Training loss: 0.65997\n",
      "Epoch: 14/40 Iteration: 132 Training loss: 0.66736\n",
      "Epoch: 14/40 Iteration: 133 Training loss: 0.65946\n",
      "Epoch: 14/40 Iteration: 134 Training loss: 0.65828\n",
      "Epoch: 13/40 Iteration: 135 Validation Acc: 0.6609\n",
      "Epoch: 14/40 Iteration: 135 Training loss: 0.65373\n",
      "Epoch: 14/40 Iteration: 136 Training loss: 0.66060\n",
      "Epoch: 14/40 Iteration: 137 Training loss: 0.66035\n",
      "Epoch: 14/40 Iteration: 138 Training loss: 0.66501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/40 Iteration: 139 Training loss: 0.65522\n",
      "Epoch: 13/40 Iteration: 140 Validation Acc: 0.6609\n",
      "Epoch: 15/40 Iteration: 140 Training loss: 0.66299\n",
      "Epoch: 15/40 Iteration: 141 Training loss: 0.65653\n",
      "Epoch: 15/40 Iteration: 142 Training loss: 0.66492\n",
      "Epoch: 15/40 Iteration: 143 Training loss: 0.65683\n",
      "Epoch: 15/40 Iteration: 144 Training loss: 0.65556\n",
      "Epoch: 14/40 Iteration: 145 Validation Acc: 0.6593\n",
      "Epoch: 15/40 Iteration: 145 Training loss: 0.65046\n",
      "Epoch: 15/40 Iteration: 146 Training loss: 0.65844\n",
      "Epoch: 15/40 Iteration: 147 Training loss: 0.65788\n",
      "Epoch: 15/40 Iteration: 148 Training loss: 0.66293\n",
      "Epoch: 15/40 Iteration: 149 Training loss: 0.65207\n",
      "Epoch: 14/40 Iteration: 150 Validation Acc: 0.6593\n",
      "Epoch: 16/40 Iteration: 150 Training loss: 0.66043\n",
      "Epoch: 16/40 Iteration: 151 Training loss: 0.65366\n",
      "Epoch: 16/40 Iteration: 152 Training loss: 0.66261\n",
      "Epoch: 16/40 Iteration: 153 Training loss: 0.65406\n",
      "Epoch: 16/40 Iteration: 154 Training loss: 0.65285\n",
      "Epoch: 15/40 Iteration: 155 Validation Acc: 0.6593\n",
      "Epoch: 16/40 Iteration: 155 Training loss: 0.64743\n",
      "Epoch: 16/40 Iteration: 156 Training loss: 0.65633\n",
      "Epoch: 16/40 Iteration: 157 Training loss: 0.65552\n",
      "Epoch: 16/40 Iteration: 158 Training loss: 0.66094\n",
      "Epoch: 16/40 Iteration: 159 Training loss: 0.64909\n",
      "Epoch: 15/40 Iteration: 160 Validation Acc: 0.6593\n",
      "Epoch: 17/40 Iteration: 160 Training loss: 0.65807\n",
      "Epoch: 17/40 Iteration: 161 Training loss: 0.65015\n",
      "Epoch: 17/40 Iteration: 162 Training loss: 0.66013\n",
      "Epoch: 17/40 Iteration: 163 Training loss: 0.65165\n",
      "Epoch: 17/40 Iteration: 164 Training loss: 0.65016\n",
      "Epoch: 16/40 Iteration: 165 Validation Acc: 0.6609\n",
      "Epoch: 17/40 Iteration: 165 Training loss: 0.64437\n",
      "Epoch: 17/40 Iteration: 166 Training loss: 0.65420\n",
      "Epoch: 17/40 Iteration: 167 Training loss: 0.65325\n",
      "Epoch: 17/40 Iteration: 168 Training loss: 0.65842\n",
      "Epoch: 17/40 Iteration: 169 Training loss: 0.64606\n",
      "Epoch: 16/40 Iteration: 170 Validation Acc: 0.6656\n",
      "Epoch: 18/40 Iteration: 170 Training loss: 0.65557\n",
      "Epoch: 18/40 Iteration: 171 Training loss: 0.64690\n",
      "Epoch: 18/40 Iteration: 172 Training loss: 0.65776\n",
      "Epoch: 18/40 Iteration: 173 Training loss: 0.64910\n",
      "Epoch: 18/40 Iteration: 174 Training loss: 0.64735\n",
      "Epoch: 17/40 Iteration: 175 Validation Acc: 0.6672\n",
      "Epoch: 18/40 Iteration: 175 Training loss: 0.64131\n",
      "Epoch: 18/40 Iteration: 176 Training loss: 0.65214\n",
      "Epoch: 18/40 Iteration: 177 Training loss: 0.65066\n",
      "Epoch: 18/40 Iteration: 178 Training loss: 0.65557\n",
      "Epoch: 18/40 Iteration: 179 Training loss: 0.64298\n",
      "Epoch: 17/40 Iteration: 180 Validation Acc: 0.6719\n",
      "Epoch: 19/40 Iteration: 180 Training loss: 0.65267\n",
      "Epoch: 19/40 Iteration: 181 Training loss: 0.64289\n",
      "Epoch: 19/40 Iteration: 182 Training loss: 0.65536\n",
      "Epoch: 19/40 Iteration: 183 Training loss: 0.64710\n",
      "Epoch: 19/40 Iteration: 184 Training loss: 0.64392\n",
      "Epoch: 18/40 Iteration: 185 Validation Acc: 0.6767\n",
      "Epoch: 19/40 Iteration: 185 Training loss: 0.63876\n",
      "Epoch: 19/40 Iteration: 186 Training loss: 0.65019\n",
      "Epoch: 19/40 Iteration: 187 Training loss: 0.64787\n",
      "Epoch: 19/40 Iteration: 188 Training loss: 0.65235\n",
      "Epoch: 19/40 Iteration: 189 Training loss: 0.63966\n",
      "Epoch: 18/40 Iteration: 190 Validation Acc: 0.6782\n",
      "Epoch: 20/40 Iteration: 190 Training loss: 0.64908\n",
      "Epoch: 20/40 Iteration: 191 Training loss: 0.63929\n",
      "Epoch: 20/40 Iteration: 192 Training loss: 0.65196\n",
      "Epoch: 20/40 Iteration: 193 Training loss: 0.64485\n",
      "Epoch: 20/40 Iteration: 194 Training loss: 0.64064\n",
      "Epoch: 19/40 Iteration: 195 Validation Acc: 0.6767\n",
      "Epoch: 20/40 Iteration: 195 Training loss: 0.63611\n",
      "Epoch: 20/40 Iteration: 196 Training loss: 0.64693\n",
      "Epoch: 20/40 Iteration: 197 Training loss: 0.64371\n",
      "Epoch: 20/40 Iteration: 198 Training loss: 0.64926\n",
      "Epoch: 20/40 Iteration: 199 Training loss: 0.63396\n",
      "Epoch: 19/40 Iteration: 200 Validation Acc: 0.6782\n",
      "Epoch: 21/40 Iteration: 200 Training loss: 0.64378\n",
      "Epoch: 21/40 Iteration: 201 Training loss: 0.63385\n",
      "Epoch: 21/40 Iteration: 202 Training loss: 0.64643\n",
      "Epoch: 21/40 Iteration: 203 Training loss: 0.64219\n",
      "Epoch: 21/40 Iteration: 204 Training loss: 0.63567\n",
      "Epoch: 20/40 Iteration: 205 Validation Acc: 0.6830\n",
      "Epoch: 21/40 Iteration: 205 Training loss: 0.63319\n",
      "Epoch: 21/40 Iteration: 206 Training loss: 0.63841\n",
      "Epoch: 21/40 Iteration: 207 Training loss: 0.63700\n",
      "Epoch: 21/40 Iteration: 208 Training loss: 0.64411\n",
      "Epoch: 21/40 Iteration: 209 Training loss: 0.62650\n",
      "Epoch: 20/40 Iteration: 210 Validation Acc: 0.6877\n",
      "Epoch: 22/40 Iteration: 210 Training loss: 0.63691\n",
      "Epoch: 22/40 Iteration: 211 Training loss: 0.62821\n",
      "Epoch: 22/40 Iteration: 212 Training loss: 0.63585\n",
      "Epoch: 22/40 Iteration: 213 Training loss: 0.64123\n",
      "Epoch: 22/40 Iteration: 214 Training loss: 0.63276\n",
      "Epoch: 21/40 Iteration: 215 Validation Acc: 0.6972\n",
      "Epoch: 22/40 Iteration: 215 Training loss: 0.62819\n",
      "Epoch: 22/40 Iteration: 216 Training loss: 0.62734\n",
      "Epoch: 22/40 Iteration: 217 Training loss: 0.62910\n",
      "Epoch: 22/40 Iteration: 218 Training loss: 0.63419\n",
      "Epoch: 22/40 Iteration: 219 Training loss: 0.61989\n",
      "Epoch: 21/40 Iteration: 220 Validation Acc: 0.7050\n",
      "Epoch: 23/40 Iteration: 220 Training loss: 0.63135\n",
      "Epoch: 23/40 Iteration: 221 Training loss: 0.61883\n",
      "Epoch: 23/40 Iteration: 222 Training loss: 0.62514\n",
      "Epoch: 23/40 Iteration: 223 Training loss: 0.63576\n",
      "Epoch: 23/40 Iteration: 224 Training loss: 0.62765\n",
      "Epoch: 22/40 Iteration: 225 Validation Acc: 0.7066\n",
      "Epoch: 23/40 Iteration: 225 Training loss: 0.61921\n",
      "Epoch: 23/40 Iteration: 226 Training loss: 0.61720\n",
      "Epoch: 23/40 Iteration: 227 Training loss: 0.62005\n",
      "Epoch: 23/40 Iteration: 228 Training loss: 0.62356\n",
      "Epoch: 23/40 Iteration: 229 Training loss: 0.61130\n",
      "Epoch: 22/40 Iteration: 230 Validation Acc: 0.7129\n",
      "Epoch: 24/40 Iteration: 230 Training loss: 0.62399\n",
      "Epoch: 24/40 Iteration: 231 Training loss: 0.60856\n",
      "Epoch: 24/40 Iteration: 232 Training loss: 0.61452\n",
      "Epoch: 24/40 Iteration: 233 Training loss: 0.63092\n",
      "Epoch: 24/40 Iteration: 234 Training loss: 0.61883\n",
      "Epoch: 23/40 Iteration: 235 Validation Acc: 0.7224\n",
      "Epoch: 24/40 Iteration: 235 Training loss: 0.60957\n",
      "Epoch: 24/40 Iteration: 236 Training loss: 0.60837\n",
      "Epoch: 24/40 Iteration: 237 Training loss: 0.61085\n",
      "Epoch: 24/40 Iteration: 238 Training loss: 0.61513\n",
      "Epoch: 24/40 Iteration: 239 Training loss: 0.60581\n",
      "Epoch: 23/40 Iteration: 240 Validation Acc: 0.7287\n",
      "Epoch: 25/40 Iteration: 240 Training loss: 0.61780\n",
      "Epoch: 25/40 Iteration: 241 Training loss: 0.59947\n",
      "Epoch: 25/40 Iteration: 242 Training loss: 0.60350\n",
      "Epoch: 25/40 Iteration: 243 Training loss: 0.62930\n",
      "Epoch: 25/40 Iteration: 244 Training loss: 0.61309\n",
      "Epoch: 24/40 Iteration: 245 Validation Acc: 0.7240\n",
      "Epoch: 25/40 Iteration: 245 Training loss: 0.59960\n",
      "Epoch: 25/40 Iteration: 246 Training loss: 0.59858\n",
      "Epoch: 25/40 Iteration: 247 Training loss: 0.60147\n",
      "Epoch: 25/40 Iteration: 248 Training loss: 0.60638\n",
      "Epoch: 25/40 Iteration: 249 Training loss: 0.59864\n",
      "Epoch: 24/40 Iteration: 250 Validation Acc: 0.7271\n",
      "Epoch: 26/40 Iteration: 250 Training loss: 0.61196\n",
      "Epoch: 26/40 Iteration: 251 Training loss: 0.59431\n",
      "Epoch: 26/40 Iteration: 252 Training loss: 0.59370\n",
      "Epoch: 26/40 Iteration: 253 Training loss: 0.61900\n",
      "Epoch: 26/40 Iteration: 254 Training loss: 0.60636\n",
      "Epoch: 25/40 Iteration: 255 Validation Acc: 0.7224\n",
      "Epoch: 26/40 Iteration: 255 Training loss: 0.59156\n",
      "Epoch: 26/40 Iteration: 256 Training loss: 0.58956\n",
      "Epoch: 26/40 Iteration: 257 Training loss: 0.59105\n",
      "Epoch: 26/40 Iteration: 258 Training loss: 0.59529\n",
      "Epoch: 26/40 Iteration: 259 Training loss: 0.58947\n",
      "Epoch: 25/40 Iteration: 260 Validation Acc: 0.7350\n",
      "Epoch: 27/40 Iteration: 260 Training loss: 0.60477\n",
      "Epoch: 27/40 Iteration: 261 Training loss: 0.58895\n",
      "Epoch: 27/40 Iteration: 262 Training loss: 0.58343\n",
      "Epoch: 27/40 Iteration: 263 Training loss: 0.60875\n",
      "Epoch: 27/40 Iteration: 264 Training loss: 0.59507\n",
      "Epoch: 26/40 Iteration: 265 Validation Acc: 0.7271\n",
      "Epoch: 27/40 Iteration: 265 Training loss: 0.58233\n",
      "Epoch: 27/40 Iteration: 266 Training loss: 0.57960\n",
      "Epoch: 27/40 Iteration: 267 Training loss: 0.58138\n",
      "Epoch: 27/40 Iteration: 268 Training loss: 0.58730\n",
      "Epoch: 27/40 Iteration: 269 Training loss: 0.58176\n",
      "Epoch: 26/40 Iteration: 270 Validation Acc: 0.7303\n",
      "Epoch: 28/40 Iteration: 270 Training loss: 0.59861\n",
      "Epoch: 28/40 Iteration: 271 Training loss: 0.58131\n",
      "Epoch: 28/40 Iteration: 272 Training loss: 0.57578\n",
      "Epoch: 28/40 Iteration: 273 Training loss: 0.59620\n",
      "Epoch: 28/40 Iteration: 274 Training loss: 0.58494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27/40 Iteration: 275 Validation Acc: 0.7382\n",
      "Epoch: 28/40 Iteration: 275 Training loss: 0.57274\n",
      "Epoch: 28/40 Iteration: 276 Training loss: 0.57032\n",
      "Epoch: 28/40 Iteration: 277 Training loss: 0.57245\n",
      "Epoch: 28/40 Iteration: 278 Training loss: 0.57988\n",
      "Epoch: 28/40 Iteration: 279 Training loss: 0.57422\n",
      "Epoch: 27/40 Iteration: 280 Validation Acc: 0.7287\n",
      "Epoch: 29/40 Iteration: 280 Training loss: 0.59107\n",
      "Epoch: 29/40 Iteration: 281 Training loss: 0.57426\n",
      "Epoch: 29/40 Iteration: 282 Training loss: 0.56518\n",
      "Epoch: 29/40 Iteration: 283 Training loss: 0.58896\n",
      "Epoch: 29/40 Iteration: 284 Training loss: 0.57613\n",
      "Epoch: 28/40 Iteration: 285 Validation Acc: 0.7382\n",
      "Epoch: 29/40 Iteration: 285 Training loss: 0.56492\n",
      "Epoch: 29/40 Iteration: 286 Training loss: 0.56043\n",
      "Epoch: 29/40 Iteration: 287 Training loss: 0.56360\n",
      "Epoch: 29/40 Iteration: 288 Training loss: 0.57315\n",
      "Epoch: 29/40 Iteration: 289 Training loss: 0.56639\n",
      "Epoch: 28/40 Iteration: 290 Validation Acc: 0.7303\n",
      "Epoch: 30/40 Iteration: 290 Training loss: 0.58436\n",
      "Epoch: 30/40 Iteration: 291 Training loss: 0.56554\n",
      "Epoch: 30/40 Iteration: 292 Training loss: 0.55621\n",
      "Epoch: 30/40 Iteration: 293 Training loss: 0.58095\n",
      "Epoch: 30/40 Iteration: 294 Training loss: 0.56700\n",
      "Epoch: 29/40 Iteration: 295 Validation Acc: 0.7350\n",
      "Epoch: 30/40 Iteration: 295 Training loss: 0.55402\n",
      "Epoch: 30/40 Iteration: 296 Training loss: 0.55207\n",
      "Epoch: 30/40 Iteration: 297 Training loss: 0.55508\n",
      "Epoch: 30/40 Iteration: 298 Training loss: 0.56571\n",
      "Epoch: 30/40 Iteration: 299 Training loss: 0.55849\n",
      "Epoch: 29/40 Iteration: 300 Validation Acc: 0.7303\n",
      "Epoch: 31/40 Iteration: 300 Training loss: 0.57722\n",
      "Epoch: 31/40 Iteration: 301 Training loss: 0.56003\n",
      "Epoch: 31/40 Iteration: 302 Training loss: 0.54632\n",
      "Epoch: 31/40 Iteration: 303 Training loss: 0.57369\n",
      "Epoch: 31/40 Iteration: 304 Training loss: 0.55834\n",
      "Epoch: 30/40 Iteration: 305 Validation Acc: 0.7366\n",
      "Epoch: 31/40 Iteration: 305 Training loss: 0.54724\n",
      "Epoch: 31/40 Iteration: 306 Training loss: 0.54346\n",
      "Epoch: 31/40 Iteration: 307 Training loss: 0.54863\n",
      "Epoch: 31/40 Iteration: 308 Training loss: 0.55893\n",
      "Epoch: 31/40 Iteration: 309 Training loss: 0.55024\n",
      "Epoch: 30/40 Iteration: 310 Validation Acc: 0.7350\n",
      "Epoch: 32/40 Iteration: 310 Training loss: 0.57105\n",
      "Epoch: 32/40 Iteration: 311 Training loss: 0.55273\n",
      "Epoch: 32/40 Iteration: 312 Training loss: 0.53752\n",
      "Epoch: 32/40 Iteration: 313 Training loss: 0.56299\n",
      "Epoch: 32/40 Iteration: 314 Training loss: 0.54786\n",
      "Epoch: 31/40 Iteration: 315 Validation Acc: 0.7334\n",
      "Epoch: 32/40 Iteration: 315 Training loss: 0.53800\n",
      "Epoch: 32/40 Iteration: 316 Training loss: 0.53865\n",
      "Epoch: 32/40 Iteration: 317 Training loss: 0.54208\n",
      "Epoch: 32/40 Iteration: 318 Training loss: 0.55310\n",
      "Epoch: 32/40 Iteration: 319 Training loss: 0.54586\n",
      "Epoch: 31/40 Iteration: 320 Validation Acc: 0.7350\n",
      "Epoch: 33/40 Iteration: 320 Training loss: 0.56741\n",
      "Epoch: 33/40 Iteration: 321 Training loss: 0.54490\n",
      "Epoch: 33/40 Iteration: 322 Training loss: 0.52746\n",
      "Epoch: 33/40 Iteration: 323 Training loss: 0.55693\n",
      "Epoch: 33/40 Iteration: 324 Training loss: 0.54004\n",
      "Epoch: 32/40 Iteration: 325 Validation Acc: 0.7524\n",
      "Epoch: 33/40 Iteration: 325 Training loss: 0.52422\n",
      "Epoch: 33/40 Iteration: 326 Training loss: 0.52893\n",
      "Epoch: 33/40 Iteration: 327 Training loss: 0.53461\n",
      "Epoch: 33/40 Iteration: 328 Training loss: 0.54448\n",
      "Epoch: 33/40 Iteration: 329 Training loss: 0.53444\n",
      "Epoch: 32/40 Iteration: 330 Validation Acc: 0.7397\n",
      "Epoch: 34/40 Iteration: 330 Training loss: 0.55653\n",
      "Epoch: 34/40 Iteration: 331 Training loss: 0.53657\n",
      "Epoch: 34/40 Iteration: 332 Training loss: 0.51949\n",
      "Epoch: 34/40 Iteration: 333 Training loss: 0.54571\n",
      "Epoch: 34/40 Iteration: 334 Training loss: 0.53031\n",
      "Epoch: 33/40 Iteration: 335 Validation Acc: 0.7382\n",
      "Epoch: 34/40 Iteration: 335 Training loss: 0.51856\n",
      "Epoch: 34/40 Iteration: 336 Training loss: 0.52360\n",
      "Epoch: 34/40 Iteration: 337 Training loss: 0.52905\n",
      "Epoch: 34/40 Iteration: 338 Training loss: 0.53852\n",
      "Epoch: 34/40 Iteration: 339 Training loss: 0.52960\n",
      "Epoch: 33/40 Iteration: 340 Validation Acc: 0.7429\n",
      "Epoch: 35/40 Iteration: 340 Training loss: 0.55230\n",
      "Epoch: 35/40 Iteration: 341 Training loss: 0.52988\n",
      "Epoch: 35/40 Iteration: 342 Training loss: 0.51072\n",
      "Epoch: 35/40 Iteration: 343 Training loss: 0.53621\n",
      "Epoch: 35/40 Iteration: 344 Training loss: 0.52018\n",
      "Epoch: 34/40 Iteration: 345 Validation Acc: 0.7429\n",
      "Epoch: 35/40 Iteration: 345 Training loss: 0.50747\n",
      "Epoch: 35/40 Iteration: 346 Training loss: 0.51756\n",
      "Epoch: 35/40 Iteration: 347 Training loss: 0.52442\n",
      "Epoch: 35/40 Iteration: 348 Training loss: 0.53129\n",
      "Epoch: 35/40 Iteration: 349 Training loss: 0.52266\n",
      "Epoch: 34/40 Iteration: 350 Validation Acc: 0.7429\n",
      "Epoch: 36/40 Iteration: 350 Training loss: 0.54826\n",
      "Epoch: 36/40 Iteration: 351 Training loss: 0.52108\n",
      "Epoch: 36/40 Iteration: 352 Training loss: 0.50132\n",
      "Epoch: 36/40 Iteration: 353 Training loss: 0.52884\n",
      "Epoch: 36/40 Iteration: 354 Training loss: 0.51179\n",
      "Epoch: 35/40 Iteration: 355 Validation Acc: 0.7461\n",
      "Epoch: 36/40 Iteration: 355 Training loss: 0.49706\n",
      "Epoch: 36/40 Iteration: 356 Training loss: 0.50725\n",
      "Epoch: 36/40 Iteration: 357 Training loss: 0.51670\n",
      "Epoch: 36/40 Iteration: 358 Training loss: 0.52111\n",
      "Epoch: 36/40 Iteration: 359 Training loss: 0.51010\n",
      "Epoch: 35/40 Iteration: 360 Validation Acc: 0.7445\n",
      "Epoch: 37/40 Iteration: 360 Training loss: 0.53843\n",
      "Epoch: 37/40 Iteration: 361 Training loss: 0.51270\n",
      "Epoch: 37/40 Iteration: 362 Training loss: 0.49488\n",
      "Epoch: 37/40 Iteration: 363 Training loss: 0.51781\n",
      "Epoch: 37/40 Iteration: 364 Training loss: 0.50305\n",
      "Epoch: 36/40 Iteration: 365 Validation Acc: 0.7445\n",
      "Epoch: 37/40 Iteration: 365 Training loss: 0.48857\n",
      "Epoch: 37/40 Iteration: 366 Training loss: 0.50500\n",
      "Epoch: 37/40 Iteration: 367 Training loss: 0.51082\n",
      "Epoch: 37/40 Iteration: 368 Training loss: 0.51740\n",
      "Epoch: 37/40 Iteration: 369 Training loss: 0.51120\n",
      "Epoch: 36/40 Iteration: 370 Validation Acc: 0.7445\n",
      "Epoch: 38/40 Iteration: 370 Training loss: 0.53540\n",
      "Epoch: 38/40 Iteration: 371 Training loss: 0.50637\n",
      "Epoch: 38/40 Iteration: 372 Training loss: 0.48513\n",
      "Epoch: 38/40 Iteration: 373 Training loss: 0.51194\n",
      "Epoch: 38/40 Iteration: 374 Training loss: 0.49456\n",
      "Epoch: 37/40 Iteration: 375 Validation Acc: 0.7445\n",
      "Epoch: 38/40 Iteration: 375 Training loss: 0.47639\n",
      "Epoch: 38/40 Iteration: 376 Training loss: 0.49504\n",
      "Epoch: 38/40 Iteration: 377 Training loss: 0.50482\n",
      "Epoch: 38/40 Iteration: 378 Training loss: 0.50567\n",
      "Epoch: 38/40 Iteration: 379 Training loss: 0.50271\n",
      "Epoch: 37/40 Iteration: 380 Validation Acc: 0.7461\n",
      "Epoch: 39/40 Iteration: 380 Training loss: 0.53164\n",
      "Epoch: 39/40 Iteration: 381 Training loss: 0.50234\n",
      "Epoch: 39/40 Iteration: 382 Training loss: 0.47831\n",
      "Epoch: 39/40 Iteration: 383 Training loss: 0.50204\n",
      "Epoch: 39/40 Iteration: 384 Training loss: 0.48730\n",
      "Epoch: 38/40 Iteration: 385 Validation Acc: 0.7429\n",
      "Epoch: 39/40 Iteration: 385 Training loss: 0.46809\n",
      "Epoch: 39/40 Iteration: 386 Training loss: 0.48356\n",
      "Epoch: 39/40 Iteration: 387 Training loss: 0.49879\n",
      "Epoch: 39/40 Iteration: 388 Training loss: 0.49653\n",
      "Epoch: 39/40 Iteration: 389 Training loss: 0.49206\n",
      "Epoch: 38/40 Iteration: 390 Validation Acc: 0.7492\n",
      "Epoch: 40/40 Iteration: 390 Training loss: 0.52232\n",
      "Epoch: 40/40 Iteration: 391 Training loss: 0.49517\n",
      "Epoch: 40/40 Iteration: 392 Training loss: 0.47382\n",
      "Epoch: 40/40 Iteration: 393 Training loss: 0.48823\n",
      "Epoch: 40/40 Iteration: 394 Training loss: 0.47753\n",
      "Epoch: 39/40 Iteration: 395 Validation Acc: 0.7429\n",
      "Epoch: 40/40 Iteration: 395 Training loss: 0.46149\n",
      "Epoch: 40/40 Iteration: 396 Training loss: 0.47707\n",
      "Epoch: 40/40 Iteration: 397 Training loss: 0.49315\n",
      "Epoch: 40/40 Iteration: 398 Training loss: 0.49353\n",
      "Epoch: 40/40 Iteration: 399 Training loss: 0.48484\n",
      "Epoch: 39/40 Iteration: 400 Validation Acc: 0.7603\n"
     ]
    }
   ],
   "source": [
    "# 运行多少轮次\n",
    "epochs = 40\n",
    "# 统计训练效果的频率\n",
    "iteration = 0\n",
    "# 保存模型的保存器\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(epochs):\n",
    "        for x, y in get_batches(train_x, train_y):\n",
    "            feed = {inputs_: x,\n",
    "                    labels_: y}\n",
    "            # 训练模型\n",
    "            loss, _ = sess.run([cost, optimizer], feed_dict=feed)\n",
    "            print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "                  \"Iteration: {}\".format(iteration),\n",
    "                  \"Training loss: {:.5f}\".format(loss))\n",
    "            iteration += 1\n",
    "            \n",
    "            if iteration % 5 == 0:\n",
    "                feed = {inputs_: val_x,\n",
    "                        labels_: val_y}\n",
    "                val_acc = sess.run(accuracy, feed_dict=feed)\n",
    "#                 val_auc = sess.run(auc_value, feed_dict=feed)\n",
    "                # 输出用验证机验证训练进度\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Validation Acc: {:.4f}\".format(val_acc))\n",
    "    # 保存模型\n",
    "    saver.save(sess, \"checkpoints/flowers.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HanY\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from checkpoints\\flowers.ckpt\n",
      "Test accuracy: 0.8408\n",
      "auc: 0.8728\n",
      "[[ 887  774]\n",
      " [  33 3375]]\n",
      "[0.6946249  0.30537507]\n",
      "[1. 0.]\n",
      "[0.6644181  0.33558196]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.40308964 0.59691036]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.9967529  0.00324706]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.66714567 0.33285433]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.8691951  0.13080488]\n",
      "[1. 0.]\n",
      "[0.91886437 0.08113564]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34950525 0.6504947 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.39802143 0.60197854]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.4167474  0.58325255]\n",
      "[1. 0.]\n",
      "[0.76627874 0.2337213 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.43794447 0.5620555 ]\n",
      "[1. 0.]\n",
      "[0.36761034 0.6323896 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.82232064 0.1776794 ]\n",
      "[1. 0.]\n",
      "[0.871479   0.12852103]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.9262081  0.07379192]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.99773586 0.00226416]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.9282484  0.07175156]\n",
      "[1. 0.]\n",
      "[0.70332646 0.29667354]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.993163   0.00683704]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.9362454  0.06375463]\n",
      "[1. 0.]\n",
      "[0.3863732  0.61362684]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.9963955 0.0036045]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.41137752 0.5886225 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.3572089  0.64279115]\n",
      "[0. 1.]\n",
      "[0.85791    0.14208996]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.78313917 0.21686083]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.43582743 0.5641725 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.74091727 0.25908276]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.6686091  0.33139092]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.39294267 0.60705733]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.45868242 0.5413176 ]\n",
      "[0. 1.]\n",
      "[0.98639095 0.01360907]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.37977254 0.6202275 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.7506362 0.2493638]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.38291058 0.61708945]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.51190394 0.4880961 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.45742798 0.542572  ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.43372983 0.56627023]\n",
      "[1. 0.]\n",
      "[0.9462312  0.05376888]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.5797563 0.4202437]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.9583479  0.04165208]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.7547391 0.2452609]\n",
      "[1. 0.]\n",
      "[0.39352208 0.6064779 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.88853884 0.11146118]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.38578564 0.61421436]\n",
      "[0. 1.]\n",
      "[0.5478973 0.4521027]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.37588063 0.6241194 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.35354054 0.64645946]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.9202683  0.07973173]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.3753265 0.6246735]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.91366464 0.08633535]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.97317094 0.02682905]\n",
      "[1. 0.]\n",
      "[0.6142508 0.3857492]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.6569772  0.34302285]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.8316799  0.16832006]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.96631235 0.03368766]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.37373123 0.62626874]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.99374217 0.00625783]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.3893304 0.6106696]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.6456422  0.35435778]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.861395   0.13860503]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.89858323 0.10141676]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.39771906 0.6022809 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.9762597  0.02374034]\n",
      "[1. 0.]\n",
      "[0.9220956  0.07790445]\n",
      "[1. 0.]\n",
      "[0.36027455 0.6397254 ]\n",
      "[0. 1.]\n",
      "[0.35116288 0.64883715]\n",
      "[1. 0.]\n",
      "[0.9782814  0.02171862]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.6862597  0.31374025]\n",
      "[1. 0.]\n",
      "[9.9995244e-01 4.7525395e-05]\n",
      "[1. 0.]\n",
      "[0.4148451  0.58515495]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.392995   0.60700506]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.5675751  0.43242484]\n",
      "[1. 0.]\n",
      "[0.43280593 0.5671941 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.97462577 0.02537416]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.41771814 0.5822818 ]\n",
      "[1. 0.]\n",
      "[0.6997736 0.3002264]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.93922055 0.06077943]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.8149538  0.18504623]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.9037107  0.09628927]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.37198216 0.62801784]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.99715734 0.00284263]\n",
      "[1. 0.]\n",
      "[0.6980456  0.30195442]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.92235464 0.07764538]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.7346317  0.26536828]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.9557574  0.04424262]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.8296927  0.17030732]\n",
      "[1. 0.]\n",
      "[0.6479465  0.35205355]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.8734122  0.12658776]\n",
      "[1. 0.]\n",
      "[0.7263465 0.2736535]\n",
      "[1. 0.]\n",
      "[0.41965154 0.58034843]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.6644151  0.33558485]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.5243063  0.47569367]\n",
      "[0. 1.]\n",
      "[0.35346016 0.64653987]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.35392603 0.646074  ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.42587993 0.5741201 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.35638237 0.6436176 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.7800945  0.21990554]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.83093774 0.16906226]\n",
      "[1. 0.]\n",
      "[0.9254425  0.07455748]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.98994935 0.01005066]\n",
      "[1. 0.]\n",
      "[0.39879668 0.6012033 ]\n",
      "[0. 1.]\n",
      "[0.99885154 0.00114842]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.83981156 0.16018842]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.951802   0.04819801]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.96606076 0.03393923]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.55303407 0.4469659 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.80350375 0.19649628]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.44496283 0.55503714]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.55317676 0.4468232 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.9134601  0.08653989]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.7230455  0.27695447]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.6641761  0.33582386]\n",
      "[1. 0.]\n",
      "[0.7643947  0.23560527]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.65786606 0.34213394]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.4731497 0.5268503]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.99812657 0.00187339]\n",
      "[1. 0.]\n",
      "[0.43293062 0.56706935]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.51946014 0.48053983]\n",
      "[1. 0.]\n",
      "[0.96958166 0.03041841]\n",
      "[1. 0.]\n",
      "[0.50220895 0.49779108]\n",
      "[1. 0.]\n",
      "[0.9947673  0.00523271]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.62683403 0.373166  ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.46712533 0.5328747 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.5228585  0.47714153]\n",
      "[1. 0.]\n",
      "[0.3514254 0.6485746]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.74560493 0.25439507]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.7205781 0.2794219]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.41610965 0.5838904 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.85163444 0.14836556]\n",
      "[1. 0.]\n",
      "[0.6944443  0.30555567]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.9507141  0.04928591]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.9811757  0.01882422]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.99427265 0.00572732]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.92190367 0.07809636]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.8217318 0.1782682]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.473517   0.52648306]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.37036535 0.6296347 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.5168694  0.48313057]\n",
      "[1. 0.]\n",
      "[0.6921132 0.3078868]\n",
      "[1. 0.]\n",
      "[0.8745642  0.12543578]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.7187274  0.28127265]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.9211707  0.07882933]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.39930242 0.6006976 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.9415808  0.05841921]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.53866464 0.4613354 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[1. 0.]\n",
      "[0.34824315 0.6517568 ]\n",
      "[0. 1.]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    feed = {inputs_: test_x,\n",
    "            labels_: test_y}\n",
    "    test_acc = sess.run(accuracy, feed_dict=feed)\n",
    "    sess.run(auc_op, feed_dict=feed)\n",
    "    predict_test = sess.run(predicted, feed_dict=feed)\n",
    "    test_auc = sess.run(auc_value, feed_dict=feed)\n",
    "    matrix = sess.run(confusion_matrix, feed_dict = feed)\n",
    "    print(\"Test accuracy: {:.4f}\".format(test_acc))\n",
    "    print(\"auc: {:.4f}\".format(test_auc))\n",
    "    print(matrix)\n",
    "    ab = []\n",
    "    for i in range (len(test_x)):\n",
    "        xt = test_x[i][np.newaxis,:]\n",
    "        yt = test_y[i][np.newaxis,:]\n",
    "        predict_test = sess.run(predicted, feed_dict={inputs_: xt,labels_: yt})\n",
    "        ab.append(np.hstack((predict_test[0], yt[0])))\n",
    "#         print(np.hstack((predict_test[0], yt[0])))\n",
    "        print(np.array(predict_test[0]))\n",
    "        print(np.array(yt[0]))\n",
    "    np.savetxt('new2.csv',ab,delimiter =',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'End' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-6c9f13e8df6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mEnd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'End' is not defined"
     ]
    }
   ],
   "source": [
    "End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batche(x, y, n_batche=66):\n",
    "    \"\"\" 这是一个生成器函数，按照n_batches的大小将数据划分了小块 \"\"\"\n",
    "    batch_size = len(x)//n_batche\n",
    "    \n",
    "    for ii in range(0, n_batche*batch_size, batch_size):\n",
    "        # 如果不是最后一个batch，那么这个batch中应该有batch_size个数据\n",
    "        if ii != (n_batche-1)*batch_size:\n",
    "            X, Y = x[ii: ii+batch_size], y[ii: ii+batch_size] \n",
    "        # 否则的话，那剩余的不够batch_size的数据都凑入到一个batch中\n",
    "        else:\n",
    "            X, Y = x[ii:], y[ii:]\n",
    "        # 生成器语法，返回X和Y\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9971534  0.00284653]]\n",
      "[[0. 1.]]\n",
      "[[9.9987507e-01 1.2491515e-04]]\n",
      "[[0. 1.]]\n",
      "[[9.9998283e-01 1.7197459e-05]]\n",
      "[[1. 0.]]\n",
      "[[0.9918508 0.0081492]]\n",
      "[[0. 1.]]\n",
      "[[9.9997592e-01 2.4109277e-05]]\n",
      "[[0. 1.]]\n",
      "[[0.9295181  0.07048188]]\n",
      "[[1. 0.]]\n",
      "[[0.70195925 0.2980408 ]]\n",
      "[[0. 1.]]\n",
      "[[0.02803983 0.9719601 ]]\n",
      "[[0. 1.]]\n",
      "[[0.9425304 0.0574696]]\n",
      "[[0. 1.]]\n",
      "[[0.9598435  0.04015645]]\n",
      "[[1. 0.]]\n",
      "[[9.998512e-01 1.487142e-04]]\n",
      "[[1. 0.]]\n",
      "[[0.9317851  0.06821484]]\n",
      "[[0. 1.]]\n",
      "[[0.34041864 0.65958136]]\n",
      "[[1. 0.]]\n",
      "[[0.9114859  0.08851413]]\n",
      "[[0. 1.]]\n",
      "[[0.99607134 0.00392862]]\n",
      "[[1. 0.]]\n",
      "[[0.99667335 0.00332668]]\n",
      "[[1. 0.]]\n",
      "[[0.9922346  0.00776544]]\n",
      "[[1. 0.]]\n",
      "[[0.9984875  0.00151251]]\n",
      "[[1. 0.]]\n",
      "[[0.04986914 0.95013094]]\n",
      "[[1. 0.]]\n",
      "[[9.9988449e-01 1.1545216e-04]]\n",
      "[[0. 1.]]\n",
      "[[9.9928361e-01 7.1637915e-04]]\n",
      "[[0. 1.]]\n",
      "[[0.9953969  0.00460304]]\n",
      "[[0. 1.]]\n",
      "[[0.19074014 0.80925983]]\n",
      "[[1. 0.]]\n",
      "[[0.94884866 0.05115132]]\n",
      "[[1. 0.]]\n",
      "[[0.95150846 0.04849156]]\n",
      "[[1. 0.]]\n",
      "[[0.05749023 0.9425097 ]]\n",
      "[[0. 1.]]\n",
      "[[0.99823207 0.001768  ]]\n",
      "[[1. 0.]]\n",
      "[[9.9982917e-01 1.7075386e-04]]\n",
      "[[1. 0.]]\n",
      "[[0.44066146 0.5593385 ]]\n",
      "[[0. 1.]]\n",
      "[[9.9957675e-01 4.2319510e-04]]\n",
      "[[1. 0.]]\n",
      "[[0.3584955 0.6415045]]\n",
      "[[1. 0.]]\n",
      "[[9.9997079e-01 2.9256691e-05]]\n",
      "[[0. 1.]]\n",
      "[[0.9967385  0.00326155]]\n",
      "[[0. 1.]]\n",
      "[[0.9935528  0.00644724]]\n",
      "[[1. 0.]]\n",
      "[[0.21937357 0.7806265 ]]\n",
      "[[0. 1.]]\n",
      "[[0.9983032  0.00169676]]\n",
      "[[0. 1.]]\n",
      "[[0.9874467  0.01255324]]\n",
      "[[0. 1.]]\n",
      "[[0.06691669 0.9330833 ]]\n",
      "[[0. 1.]]\n",
      "[[0.9989267  0.00107331]]\n",
      "[[1. 0.]]\n",
      "[[9.995715e-01 4.285421e-04]]\n",
      "[[0. 1.]]\n",
      "[[0.9266127  0.07338732]]\n",
      "[[1. 0.]]\n",
      "[[0.14173497 0.85826504]]\n",
      "[[0. 1.]]\n",
      "[[0.9700426 0.0299574]]\n",
      "[[1. 0.]]\n",
      "[[9.9998403e-01 1.6012564e-05]]\n",
      "[[1. 0.]]\n",
      "[[0.99700826 0.00299173]]\n",
      "[[0. 1.]]\n",
      "[[0.7527717  0.24722832]]\n",
      "[[0. 1.]]\n",
      "[[9.9993563e-01 6.4362495e-05]]\n",
      "[[1. 0.]]\n",
      "[[0.01474002 0.98526   ]]\n",
      "[[0. 1.]]\n",
      "[[0.99294174 0.00705822]]\n",
      "[[1. 0.]]\n",
      "[[9.9984360e-01 1.5638118e-04]]\n",
      "[[1. 0.]]\n",
      "[[9.9940670e-01 5.9326633e-04]]\n",
      "[[1. 0.]]\n",
      "[[0.9610496  0.03895038]]\n",
      "[[1. 0.]]\n",
      "[[0.9700824  0.02991765]]\n",
      "[[1. 0.]]\n",
      "[[9.9934691e-01 6.5310154e-04]]\n",
      "[[0. 1.]]\n",
      "[[9.9999654e-01 3.4248424e-06]]\n",
      "[[1. 0.]]\n",
      "[[0.9011594  0.09884059]]\n",
      "[[0. 1.]]\n",
      "[[0.99167436 0.00832567]]\n",
      "[[0. 1.]]\n",
      "[[0.98602885 0.01397118]]\n",
      "[[0. 1.]]\n",
      "[[0.98605067 0.01394935]]\n",
      "[[1. 0.]]\n",
      "[[0.9251736  0.07482642]]\n",
      "[[1. 0.]]\n",
      "[[0.9903867  0.00961328]]\n",
      "[[0. 1.]]\n",
      "[[9.9970227e-01 2.9774522e-04]]\n",
      "[[1. 0.]]\n",
      "[[0.99554956 0.00445041]]\n",
      "[[1. 0.]]\n",
      "[[9.991192e-01 8.808076e-04]]\n",
      "[[1. 0.]]\n",
      "[[0.9706104  0.02938967]]\n",
      "[[1. 0.]]\n",
      "[[0.9878292  0.01217077]]\n",
      "[[0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    ab = []\n",
    "    for x, y in get_batche(test_x, test_y):\n",
    "            feed = {inputs_: x,\n",
    "                    labels_: y}\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "            predict_test = sess.run(predicted, feed_dict=feed)\n",
    "            ab.append(np.hstack((predict_test[0], y[0])))\n",
    "#             print(np.hstack((predict_test[0], y[0])))\n",
    "            print(predict_test)\n",
    "            print(np.array(y))\n",
    "\n",
    "#     np.savetxt('new2.csv',ab,delimiter =',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.9715097e-05 9.9993026e-01]\n",
      "[0. 1.]\n",
      "[2.2344158e-05 9.9997771e-01]\n",
      "[1. 0.]\n",
      "[1.8254350e-05 9.9998176e-01]\n",
      "[1. 0.]\n",
      "[0.9185695 0.0814305]\n",
      "[0. 1.]\n",
      "[0.05671438 0.94328564]\n",
      "[1. 0.]\n",
      "[5.0058536e-04 9.9949944e-01]\n",
      "[1. 0.]\n",
      "[0.11746989 0.88253015]\n",
      "[1. 0.]\n",
      "[4.8438529e-04 9.9951565e-01]\n",
      "[1. 0.]\n",
      "[0.81241965 0.1875804 ]\n",
      "[0. 1.]\n",
      "[2.8460303e-07 9.9999976e-01]\n",
      "[0. 1.]\n",
      "[0.20918503 0.790815  ]\n",
      "[0. 1.]\n",
      "[8.779239e-08 9.999999e-01]\n",
      "[0. 1.]\n",
      "[4.856894e-06 9.999951e-01]\n",
      "[1. 0.]\n",
      "[3.5028432e-05 9.9996495e-01]\n",
      "[0. 1.]\n",
      "[0.95642155 0.04357842]\n",
      "[0. 1.]\n",
      "[0.00126926 0.9987307 ]\n",
      "[0. 1.]\n",
      "[0.43894693 0.5610531 ]\n",
      "[0. 1.]\n",
      "[0.07156464 0.9284353 ]\n",
      "[1. 0.]\n",
      "[0.00409978 0.9959002 ]\n",
      "[0. 1.]\n",
      "[0.00803164 0.99196833]\n",
      "[0. 1.]\n",
      "[0.00304954 0.99695045]\n",
      "[0. 1.]\n",
      "[0.31845787 0.68154216]\n",
      "[1. 0.]\n",
      "[0.0022448 0.9977552]\n",
      "[0. 1.]\n",
      "[0.4045453 0.5954547]\n",
      "[1. 0.]\n",
      "[0.748622 0.251378]\n",
      "[0. 1.]\n",
      "[2.3592493e-04 9.9976403e-01]\n",
      "[0. 1.]\n",
      "[0.00563512 0.99436486]\n",
      "[1. 0.]\n",
      "[0.53675324 0.46324673]\n",
      "[0. 1.]\n",
      "[6.741014e-04 9.993259e-01]\n",
      "[0. 1.]\n",
      "[0.11045805 0.8895419 ]\n",
      "[0. 1.]\n",
      "[0.00281177 0.99718827]\n",
      "[0. 1.]\n",
      "[0.00712592 0.992874  ]\n",
      "[0. 1.]\n",
      "[0.00363595 0.99636406]\n",
      "[1. 0.]\n",
      "[0.03905094 0.9609491 ]\n",
      "[1. 0.]\n",
      "[0.12557168 0.87442833]\n",
      "[1. 0.]\n",
      "[0.07725842 0.9227416 ]\n",
      "[0. 1.]\n",
      "[1.8596492e-04 9.9981409e-01]\n",
      "[0. 1.]\n",
      "[4.9679006e-06 9.9999499e-01]\n",
      "[0. 1.]\n",
      "[0.4808524 0.5191476]\n",
      "[1. 0.]\n",
      "[0.00694278 0.99305725]\n",
      "[1. 0.]\n",
      "[0.02539138 0.97460854]\n",
      "[0. 1.]\n",
      "[9.022507e-04 9.990978e-01]\n",
      "[0. 1.]\n",
      "[0.00347687 0.99652314]\n",
      "[1. 0.]\n",
      "[5.215022e-05 9.999479e-01]\n",
      "[0. 1.]\n",
      "[0.18951872 0.8104813 ]\n",
      "[1. 0.]\n",
      "[0.3214095  0.67859054]\n",
      "[1. 0.]\n",
      "[0.16436696 0.835633  ]\n",
      "[0. 1.]\n",
      "[0.06903449 0.9309655 ]\n",
      "[0. 1.]\n",
      "[0.06402842 0.9359716 ]\n",
      "[0. 1.]\n",
      "[0.23864245 0.76135755]\n",
      "[0. 1.]\n",
      "[0.00255791 0.9974421 ]\n",
      "[0. 1.]\n",
      "[0.03102332 0.9689766 ]\n",
      "[0. 1.]\n",
      "[0.04840519 0.9515948 ]\n",
      "[0. 1.]\n",
      "[0.03946981 0.9605302 ]\n",
      "[0. 1.]\n",
      "[3.7543457e-05 9.9996245e-01]\n",
      "[0. 1.]\n",
      "[4.1490362e-04 9.9958509e-01]\n",
      "[1. 0.]\n",
      "[0.01889626 0.9811038 ]\n",
      "[0. 1.]\n",
      "[0.03307155 0.9669284 ]\n",
      "[1. 0.]\n",
      "[2.0119207e-04 9.9979883e-01]\n",
      "[1. 0.]\n",
      "[0.00283663 0.99716336]\n",
      "[0. 1.]\n",
      "[0.22843073 0.77156925]\n",
      "[0. 1.]\n",
      "[0.02229189 0.97770816]\n",
      "[1. 0.]\n",
      "[0.21727678 0.7827232 ]\n",
      "[0. 1.]\n",
      "[0.04307808 0.9569219 ]\n",
      "[1. 0.]\n",
      "[7.4769865e-04 9.9925226e-01]\n",
      "[1. 0.]\n",
      "[0.0034034  0.99659663]\n",
      "[0. 1.]\n",
      "[0.13838781 0.8616122 ]\n",
      "[0. 1.]\n",
      "[1.8750961e-05 9.9998128e-01]\n",
      "[0. 1.]\n",
      "[0.24079609 0.7592039 ]\n",
      "[1. 0.]\n",
      "[0.10946274 0.89053726]\n",
      "[1. 0.]\n",
      "[8.5532916e-04 9.9914467e-01]\n",
      "[1. 0.]\n",
      "[0.91815406 0.0818459 ]\n",
      "[1. 0.]\n",
      "[0.00640836 0.9935916 ]\n",
      "[1. 0.]\n",
      "[0.38625994 0.6137401 ]\n",
      "[0. 1.]\n",
      "[1.0129623e-04 9.9989867e-01]\n",
      "[1. 0.]\n",
      "[0.16022816 0.8397718 ]\n",
      "[1. 0.]\n",
      "[2.9816006e-06 9.9999702e-01]\n",
      "[1. 0.]\n",
      "[4.3881424e-05 9.9995613e-01]\n",
      "[1. 0.]\n",
      "[4.7407387e-04 9.9952590e-01]\n",
      "[1. 0.]\n",
      "[0.6326295  0.36737052]\n",
      "[0. 1.]\n",
      "[0.00530317 0.99469686]\n",
      "[0. 1.]\n",
      "[0.65823805 0.34176192]\n",
      "[1. 0.]\n",
      "[0.01948567 0.98051435]\n",
      "[1. 0.]\n",
      "[6.4905740e-05 9.9993503e-01]\n",
      "[0. 1.]\n",
      "[0.36621776 0.63378227]\n",
      "[0. 1.]\n",
      "[0.01012197 0.98987806]\n",
      "[0. 1.]\n",
      "[4.1014442e-05 9.9995899e-01]\n",
      "[1. 0.]\n",
      "[6.9712399e-04 9.9930286e-01]\n",
      "[1. 0.]\n",
      "[0.00193067 0.99806935]\n",
      "[1. 0.]\n",
      "[0.39905193 0.60094804]\n",
      "[1. 0.]\n",
      "[0.00905498 0.990945  ]\n",
      "[1. 0.]\n",
      "[0.140028 0.859972]\n",
      "[0. 1.]\n",
      "[0.02289437 0.9771056 ]\n",
      "[0. 1.]\n",
      "[0.3027221 0.6972779]\n",
      "[0. 1.]\n",
      "[0.09161628 0.9083838 ]\n",
      "[0. 1.]\n",
      "[0.00194235 0.99805766]\n",
      "[0. 1.]\n",
      "[0.3452231  0.65477693]\n",
      "[0. 1.]\n",
      "[0.7830701 0.2169299]\n",
      "[1. 0.]\n",
      "[0.00261464 0.9973853 ]\n",
      "[0. 1.]\n",
      "[0.04184917 0.95815086]\n",
      "[0. 1.]\n",
      "[0.6758005  0.32419953]\n",
      "[0. 1.]\n",
      "[0.22341155 0.77658844]\n",
      "[0. 1.]\n",
      "[0.01865095 0.98134905]\n",
      "[1. 0.]\n",
      "[3.5384367e-04 9.9964619e-01]\n",
      "[0. 1.]\n",
      "[0.20421466 0.7957853 ]\n",
      "[1. 0.]\n",
      "[6.012556e-04 9.993987e-01]\n",
      "[1. 0.]\n",
      "[0.4162385  0.58376145]\n",
      "[0. 1.]\n",
      "[0.08727925 0.91272074]\n",
      "[0. 1.]\n",
      "[0.01453387 0.9854661 ]\n",
      "[1. 0.]\n",
      "[0.01733451 0.9826654 ]\n",
      "[0. 1.]\n",
      "[3.5991045e-05 9.9996400e-01]\n",
      "[1. 0.]\n",
      "[9.730838e-04 9.990269e-01]\n",
      "[1. 0.]\n",
      "[0.66333616 0.33666384]\n",
      "[1. 0.]\n",
      "[0.02084164 0.97915834]\n",
      "[0. 1.]\n",
      "[4.8738142e-04 9.9951267e-01]\n",
      "[1. 0.]\n",
      "[4.994253e-04 9.995005e-01]\n",
      "[1. 0.]\n",
      "[9.9739202e-07 9.9999905e-01]\n",
      "[1. 0.]\n",
      "[4.1672742e-04 9.9958330e-01]\n",
      "[0. 1.]\n",
      "[0.02612837 0.97387165]\n",
      "[0. 1.]\n",
      "[0.00137123 0.99862874]\n",
      "[0. 1.]\n",
      "[0.02630566 0.97369426]\n",
      "[0. 1.]\n",
      "[1.0818969e-06 9.9999893e-01]\n",
      "[1. 0.]\n",
      "[0.00375346 0.9962465 ]\n",
      "[0. 1.]\n",
      "[0.03320393 0.96679604]\n",
      "[0. 1.]\n",
      "[0.00110368 0.99889624]\n",
      "[0. 1.]\n",
      "[0.01415189 0.98584807]\n",
      "[1. 0.]\n",
      "[0.41684517 0.58315486]\n",
      "[1. 0.]\n",
      "[2.3408294e-04 9.9976593e-01]\n",
      "[0. 1.]\n",
      "[0.07459153 0.92540854]\n",
      "[0. 1.]\n",
      "[0.2672291 0.7327709]\n",
      "[1. 0.]\n",
      "[0.04923719 0.9507628 ]\n",
      "[0. 1.]\n",
      "[0.09142175 0.9085783 ]\n",
      "[0. 1.]\n",
      "[0.01481677 0.9851833 ]\n",
      "[0. 1.]\n",
      "[6.3667190e-04 9.9936336e-01]\n",
      "[0. 1.]\n",
      "[0.02730944 0.9726906 ]\n",
      "[0. 1.]\n",
      "[7.619150e-05 9.999238e-01]\n",
      "[0. 1.]\n",
      "[2.9220553e-05 9.9997079e-01]\n",
      "[0. 1.]\n",
      "[0.7580533 0.2419467]\n",
      "[0. 1.]\n",
      "[0.9622016  0.03779844]\n",
      "[1. 0.]\n",
      "[0.5098394 0.4901606]\n",
      "[0. 1.]\n",
      "[0.0042916  0.99570835]\n",
      "[0. 1.]\n",
      "[0.6644286  0.33557144]\n",
      "[0. 1.]\n",
      "[7.2898415e-06 9.9999273e-01]\n",
      "[0. 1.]\n",
      "[0.8779891  0.12201092]\n",
      "[1. 0.]\n",
      "[0.00855555 0.9914444 ]\n",
      "[1. 0.]\n",
      "[1.2302169e-04 9.9987698e-01]\n",
      "[1. 0.]\n",
      "[0.6496576  0.35034242]\n",
      "[1. 0.]\n",
      "[0.02690807 0.9730919 ]\n",
      "[1. 0.]\n",
      "[0.02742466 0.9725753 ]\n",
      "[1. 0.]\n",
      "[0.2258527 0.7741473]\n",
      "[1. 0.]\n",
      "[7.7981921e-04 9.9922013e-01]\n",
      "[0. 1.]\n",
      "[2.769555e-04 9.997230e-01]\n",
      "[1. 0.]\n",
      "[1.891636e-04 9.998109e-01]\n",
      "[0. 1.]\n",
      "[1.0198817e-04 9.9989796e-01]\n",
      "[0. 1.]\n",
      "[1.3658199e-04 9.9986339e-01]\n",
      "[1. 0.]\n",
      "[0.63138133 0.36861864]\n",
      "[1. 0.]\n",
      "[7.3596399e-05 9.9992645e-01]\n",
      "[1. 0.]\n",
      "[5.9412439e-05 9.9994063e-01]\n",
      "[1. 0.]\n",
      "[0.08969546 0.9103045 ]\n",
      "[0. 1.]\n",
      "[0.04246217 0.95753783]\n",
      "[0. 1.]\n",
      "[3.7684935e-05 9.9996233e-01]\n",
      "[0. 1.]\n",
      "[0.01614049 0.98385954]\n",
      "[0. 1.]\n",
      "[0.02091913 0.97908086]\n",
      "[0. 1.]\n",
      "[0.7570509  0.24294919]\n",
      "[0. 1.]\n",
      "[0.41100132 0.58899873]\n",
      "[1. 0.]\n",
      "[0.95198673 0.04801328]\n",
      "[0. 1.]\n",
      "[8.2147622e-04 9.9917847e-01]\n",
      "[1. 0.]\n",
      "[0.08162878 0.9183712 ]\n",
      "[0. 1.]\n",
      "[0.02865462 0.9713454 ]\n",
      "[1. 0.]\n",
      "[8.709012e-04 9.991291e-01]\n",
      "[0. 1.]\n",
      "[0.00534077 0.99465925]\n",
      "[0. 1.]\n",
      "[0.00190061 0.9980994 ]\n",
      "[0. 1.]\n",
      "[0.02372412 0.9762759 ]\n",
      "[0. 1.]\n",
      "[0.00525059 0.99474937]\n",
      "[1. 0.]\n",
      "[0.04027011 0.95972985]\n",
      "[0. 1.]\n",
      "[0.45703351 0.5429665 ]\n",
      "[0. 1.]\n",
      "[4.4382788e-04 9.9955612e-01]\n",
      "[0. 1.]\n",
      "[0.03206979 0.96793026]\n",
      "[1. 0.]\n",
      "[1.373058e-04 9.998627e-01]\n",
      "[0. 1.]\n",
      "[0.00419374 0.9958062 ]\n",
      "[1. 0.]\n",
      "[0.00784225 0.9921577 ]\n",
      "[0. 1.]\n",
      "[0.02437638 0.9756236 ]\n",
      "[0. 1.]\n",
      "[2.394040e-06 9.999976e-01]\n",
      "[0. 1.]\n",
      "[0.98720574 0.01279431]\n",
      "[0. 1.]\n",
      "[0.01707281 0.98292714]\n",
      "[0. 1.]\n",
      "[0.00756947 0.9924305 ]\n",
      "[0. 1.]\n",
      "[0.8310436  0.16895643]\n",
      "[0. 1.]\n",
      "[1.057874e-05 9.999894e-01]\n",
      "[1. 0.]\n",
      "[0.01841121 0.98158884]\n",
      "[0. 1.]\n",
      "[0.00611888 0.9938811 ]\n",
      "[0. 1.]\n",
      "[0.00119861 0.99880135]\n",
      "[0. 1.]\n",
      "[0.43042904 0.56957096]\n",
      "[0. 1.]\n",
      "[0.11370679 0.8862933 ]\n",
      "[0. 1.]\n",
      "[1.159755e-04 9.998840e-01]\n",
      "[0. 1.]\n",
      "[5.669232e-07 9.999994e-01]\n",
      "[0. 1.]\n",
      "[0.14598155 0.85401845]\n",
      "[1. 0.]\n",
      "[0.08983631 0.91016364]\n",
      "[1. 0.]\n",
      "[1.5644857e-04 9.9984348e-01]\n",
      "[0. 1.]\n",
      "[1.165679e-04 9.998834e-01]\n",
      "[1. 0.]\n",
      "[0.55413586 0.44586414]\n",
      "[1. 0.]\n",
      "[0.0127683 0.9872317]\n",
      "[1. 0.]\n",
      "[0.2788107  0.72118926]\n",
      "[0. 1.]\n",
      "[0.14623648 0.8537635 ]\n",
      "[0. 1.]\n",
      "[0.00626782 0.99373215]\n",
      "[1. 0.]\n",
      "[0.00575787 0.99424213]\n",
      "[0. 1.]\n",
      "[0.02215466 0.9778454 ]\n",
      "[0. 1.]\n",
      "[0.0064539  0.99354607]\n",
      "[1. 0.]\n",
      "[7.460015e-04 9.992539e-01]\n",
      "[0. 1.]\n",
      "[0.00402743 0.9959726 ]\n",
      "[0. 1.]\n",
      "[0.15779746 0.8422025 ]\n",
      "[0. 1.]\n",
      "[0.10713989 0.8928601 ]\n",
      "[1. 0.]\n",
      "[0.0015512 0.9984487]\n",
      "[0. 1.]\n",
      "[0.6306564  0.36934364]\n",
      "[1. 0.]\n",
      "[0.02244394 0.97755605]\n",
      "[0. 1.]\n",
      "[0.20455182 0.7954482 ]\n",
      "[1. 0.]\n",
      "[0.13981427 0.86018574]\n",
      "[1. 0.]\n",
      "[0.00444184 0.99555814]\n",
      "[0. 1.]\n",
      "[2.7550440e-04 9.9972445e-01]\n",
      "[0. 1.]\n",
      "[0.00119322 0.99880683]\n",
      "[0. 1.]\n",
      "[0.01419819 0.9858019 ]\n",
      "[0. 1.]\n",
      "[0.24754772 0.7524523 ]\n",
      "[1. 0.]\n",
      "[5.5221253e-04 9.9944776e-01]\n",
      "[1. 0.]\n",
      "[1.14303824e-04 9.99885678e-01]\n",
      "[1. 0.]\n",
      "[0.00239257 0.99760747]\n",
      "[1. 0.]\n",
      "[0.5790947  0.42090535]\n",
      "[1. 0.]\n",
      "[0.01677091 0.9832291 ]\n",
      "[0. 1.]\n",
      "[0.10188808 0.89811194]\n",
      "[1. 0.]\n",
      "[3.0937250e-04 9.9969065e-01]\n",
      "[1. 0.]\n",
      "[6.9776305e-04 9.9930227e-01]\n",
      "[1. 0.]\n",
      "[0.4004087  0.59959126]\n",
      "[0. 1.]\n",
      "[0.7408377 0.2591623]\n",
      "[1. 0.]\n",
      "[0.00643064 0.9935694 ]\n",
      "[0. 1.]\n",
      "[0.27996698 0.72003305]\n",
      "[0. 1.]\n",
      "[0.10508838 0.89491165]\n",
      "[0. 1.]\n",
      "[0.05357973 0.94642025]\n",
      "[0. 1.]\n",
      "[0.28961748 0.7103825 ]\n",
      "[0. 1.]\n",
      "[0.01555852 0.98444146]\n",
      "[0. 1.]\n",
      "[0.01360551 0.9863945 ]\n",
      "[0. 1.]\n",
      "[0.96444494 0.03555505]\n",
      "[0. 1.]\n",
      "[0.00188799 0.99811196]\n",
      "[0. 1.]\n",
      "[1.3829615e-04 9.9986172e-01]\n",
      "[1. 0.]\n",
      "[0.00124242 0.9987576 ]\n",
      "[0. 1.]\n",
      "[2.7645727e-05 9.9997234e-01]\n",
      "[0. 1.]\n",
      "[0.02373324 0.97626674]\n",
      "[0. 1.]\n",
      "[0.00124831 0.99875164]\n",
      "[0. 1.]\n",
      "[0.02447636 0.97552365]\n",
      "[0. 1.]\n",
      "[2.0481154e-04 9.9979526e-01]\n",
      "[0. 1.]\n",
      "[0.54998946 0.45001048]\n",
      "[0. 1.]\n",
      "[0.35090387 0.64909613]\n",
      "[0. 1.]\n",
      "[0.97241634 0.02758365]\n",
      "[0. 1.]\n",
      "[0.8574668  0.14253321]\n",
      "[0. 1.]\n",
      "[0.00952815 0.99047184]\n",
      "[0. 1.]\n",
      "[0.01562207 0.9843779 ]\n",
      "[1. 0.]\n",
      "[4.0001156e-05 9.9995995e-01]\n",
      "[1. 0.]\n",
      "[0.00472425 0.99527574]\n",
      "[0. 1.]\n",
      "[0.00244329 0.9975567 ]\n",
      "[0. 1.]\n",
      "[0.19675697 0.803243  ]\n",
      "[0. 1.]\n",
      "[0.00305022 0.99694985]\n",
      "[1. 0.]\n",
      "[1.5291976e-04 9.9984705e-01]\n",
      "[0. 1.]\n",
      "[0.04201324 0.9579867 ]\n",
      "[0. 1.]\n",
      "[0.30779764 0.6922024 ]\n",
      "[0. 1.]\n",
      "[0.00123898 0.99876106]\n",
      "[1. 0.]\n",
      "[8.548839e-06 9.999914e-01]\n",
      "[0. 1.]\n",
      "[0.11318365 0.8868164 ]\n",
      "[0. 1.]\n",
      "[0.2654502 0.7345498]\n",
      "[0. 1.]\n",
      "[0.0206478 0.9793522]\n",
      "[1. 0.]\n",
      "[2.6130167e-05 9.9997389e-01]\n",
      "[1. 0.]\n",
      "[0.1189846  0.88101536]\n",
      "[0. 1.]\n",
      "[0.01927407 0.98072594]\n",
      "[0. 1.]\n",
      "[2.7130086e-06 9.9999726e-01]\n",
      "[1. 0.]\n",
      "[0.01826227 0.9817377 ]\n",
      "[0. 1.]\n",
      "[0.10267482 0.8973252 ]\n",
      "[0. 1.]\n",
      "[0.01582344 0.9841765 ]\n",
      "[0. 1.]\n",
      "[0.00593498 0.99406505]\n",
      "[0. 1.]\n",
      "[7.1205213e-05 9.9992883e-01]\n",
      "[1. 0.]\n",
      "[0.0170843  0.98291576]\n",
      "[0. 1.]\n",
      "[0.03455039 0.9654496 ]\n",
      "[1. 0.]\n",
      "[0.1011174 0.8988826]\n",
      "[0. 1.]\n",
      "[0.08491567 0.9150843 ]\n",
      "[0. 1.]\n",
      "[0.7308742 0.2691258]\n",
      "[0. 1.]\n",
      "[0.01333587 0.9866642 ]\n",
      "[0. 1.]\n",
      "[0.00687007 0.9931299 ]\n",
      "[0. 1.]\n",
      "[2.9342322e-04 9.9970657e-01]\n",
      "[1. 0.]\n",
      "[0.0015031 0.9984969]\n",
      "[0. 1.]\n",
      "[4.395162e-04 9.995604e-01]\n",
      "[0. 1.]\n",
      "[0.20416495 0.7958351 ]\n",
      "[1. 0.]\n",
      "[0.05987205 0.9401279 ]\n",
      "[1. 0.]\n",
      "[0.9358867  0.06411334]\n",
      "[0. 1.]\n",
      "[0.05443332 0.94556665]\n",
      "[0. 1.]\n",
      "[4.9124268e-05 9.9995089e-01]\n",
      "[1. 0.]\n",
      "[0.2971014  0.70289856]\n",
      "[1. 0.]\n",
      "[0.13396747 0.8660326 ]\n",
      "[0. 1.]\n",
      "[0.6877577  0.31224233]\n",
      "[0. 1.]\n",
      "[0.24316357 0.7568364 ]\n",
      "[0. 1.]\n",
      "[2.2389444e-04 9.9977607e-01]\n",
      "[0. 1.]\n",
      "[4.466307e-04 9.995534e-01]\n",
      "[1. 0.]\n",
      "[0.05163622 0.94836384]\n",
      "[0. 1.]\n",
      "[0.02922884 0.97077113]\n",
      "[0. 1.]\n",
      "[6.107038e-04 9.993893e-01]\n",
      "[1. 0.]\n",
      "[0.09982869 0.9001713 ]\n",
      "[0. 1.]\n",
      "[5.0079547e-05 9.9994993e-01]\n",
      "[0. 1.]\n",
      "[0.00550586 0.99449414]\n",
      "[1. 0.]\n",
      "[0.97488946 0.02511054]\n",
      "[0. 1.]\n",
      "[0.00838355 0.9916164 ]\n",
      "[0. 1.]\n",
      "[0.00992385 0.9900761 ]\n",
      "[0. 1.]\n",
      "[0.17041478 0.8295852 ]\n",
      "[0. 1.]\n",
      "[0.31374273 0.6862573 ]\n",
      "[0. 1.]\n",
      "[0.00522996 0.99477   ]\n",
      "[0. 1.]\n",
      "[0.00117778 0.99882215]\n",
      "[0. 1.]\n",
      "[1.3966049e-05 9.9998605e-01]\n",
      "[1. 0.]\n",
      "[0.00224265 0.9977574 ]\n",
      "[1. 0.]\n",
      "[3.1998084e-04 9.9968004e-01]\n",
      "[0. 1.]\n",
      "[0.00545362 0.9945464 ]\n",
      "[0. 1.]\n",
      "[0.06623147 0.93376845]\n",
      "[1. 0.]\n",
      "[0.20528924 0.7947107 ]\n",
      "[0. 1.]\n",
      "[0.01572111 0.98427886]\n",
      "[1. 0.]\n",
      "[0.0162597  0.98374027]\n",
      "[1. 0.]\n",
      "[3.773369e-05 9.999622e-01]\n",
      "[0. 1.]\n",
      "[0.06945206 0.93054795]\n",
      "[0. 1.]\n",
      "[1.8182316e-05 9.9998176e-01]\n",
      "[0. 1.]\n",
      "[2.9101334e-06 9.9999714e-01]\n",
      "[0. 1.]\n",
      "[0.25905776 0.74094224]\n",
      "[1. 0.]\n",
      "[0.3718472  0.62815285]\n",
      "[1. 0.]\n",
      "[0.01927445 0.9807256 ]\n",
      "[0. 1.]\n",
      "[0.04879088 0.9512092 ]\n",
      "[0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.46993035 0.53006965]\n",
      "[1. 0.]\n",
      "[0.02507903 0.974921  ]\n",
      "[0. 1.]\n",
      "[4.1096200e-06 9.9999595e-01]\n",
      "[0. 1.]\n",
      "[0.01796676 0.9820332 ]\n",
      "[1. 0.]\n",
      "[4.4478500e-05 9.9995553e-01]\n",
      "[0. 1.]\n",
      "[0.00374108 0.996259  ]\n",
      "[0. 1.]\n",
      "[0.43887064 0.5611294 ]\n",
      "[0. 1.]\n",
      "[0.00291158 0.9970885 ]\n",
      "[1. 0.]\n",
      "[0.938419   0.06158096]\n",
      "[0. 1.]\n",
      "[5.9418619e-04 9.9940586e-01]\n",
      "[1. 0.]\n",
      "[0.9317483  0.06825174]\n",
      "[0. 1.]\n",
      "[1.1550047e-06 9.9999881e-01]\n",
      "[0. 1.]\n",
      "[0.00437012 0.9956299 ]\n",
      "[1. 0.]\n",
      "[0.00453479 0.9954652 ]\n",
      "[1. 0.]\n",
      "[0.01724345 0.9827565 ]\n",
      "[0. 1.]\n",
      "[0.8228156  0.17718448]\n",
      "[0. 1.]\n",
      "[0.17195736 0.8280427 ]\n",
      "[0. 1.]\n",
      "[0.01441331 0.98558664]\n",
      "[0. 1.]\n",
      "[0.00308586 0.99691415]\n",
      "[0. 1.]\n",
      "[0.21808884 0.78191113]\n",
      "[0. 1.]\n",
      "[0.02263021 0.97736984]\n",
      "[1. 0.]\n",
      "[0.23644099 0.763559  ]\n",
      "[1. 0.]\n",
      "[0.03864479 0.96135527]\n",
      "[1. 0.]\n",
      "[1.538498e-04 9.998461e-01]\n",
      "[1. 0.]\n",
      "[0.19430983 0.8056901 ]\n",
      "[1. 0.]\n",
      "[0.01157933 0.9884207 ]\n",
      "[0. 1.]\n",
      "[0.03972233 0.9602776 ]\n",
      "[1. 0.]\n",
      "[0.00251645 0.99748355]\n",
      "[0. 1.]\n",
      "[0.56511813 0.43488184]\n",
      "[1. 0.]\n",
      "[2.4571772e-08 1.0000000e+00]\n",
      "[1. 0.]\n",
      "[0.00103188 0.9989681 ]\n",
      "[0. 1.]\n",
      "[0.08081582 0.91918415]\n",
      "[0. 1.]\n",
      "[0.00551743 0.99448264]\n",
      "[0. 1.]\n",
      "[5.513631e-05 9.999448e-01]\n",
      "[1. 0.]\n",
      "[0.05839939 0.9416006 ]\n",
      "[0. 1.]\n",
      "[0.0930179 0.9069821]\n",
      "[1. 0.]\n",
      "[0.00257681 0.9974232 ]\n",
      "[1. 0.]\n",
      "[5.192995e-04 9.994808e-01]\n",
      "[1. 0.]\n",
      "[0.01870226 0.98129773]\n",
      "[0. 1.]\n",
      "[0.33735815 0.6626418 ]\n",
      "[0. 1.]\n",
      "[5.433925e-04 9.994566e-01]\n",
      "[0. 1.]\n",
      "[1.8407591e-05 9.9998164e-01]\n",
      "[1. 0.]\n",
      "[0.14589535 0.8541047 ]\n",
      "[0. 1.]\n",
      "[0.03477926 0.96522075]\n",
      "[1. 0.]\n",
      "[0.01656809 0.9834319 ]\n",
      "[0. 1.]\n",
      "[0.0040065 0.9959935]\n",
      "[0. 1.]\n",
      "[0.08939083 0.9106091 ]\n",
      "[0. 1.]\n",
      "[0.85302    0.14697997]\n",
      "[0. 1.]\n",
      "[2.6040962e-05 9.9997401e-01]\n",
      "[0. 1.]\n",
      "[0.17445467 0.8255453 ]\n",
      "[1. 0.]\n",
      "[0.00149534 0.99850464]\n",
      "[0. 1.]\n",
      "[0.8386187  0.16138135]\n",
      "[0. 1.]\n",
      "[0.04406719 0.95593286]\n",
      "[1. 0.]\n",
      "[0.03427104 0.965729  ]\n",
      "[1. 0.]\n",
      "[0.5209597  0.47904032]\n",
      "[0. 1.]\n",
      "[0.01362573 0.98637426]\n",
      "[0. 1.]\n",
      "[0.19293627 0.80706376]\n",
      "[0. 1.]\n",
      "[0.00249186 0.99750817]\n",
      "[1. 0.]\n",
      "[0.3409424 0.6590576]\n",
      "[0. 1.]\n",
      "[1.5190926e-05 9.9998486e-01]\n",
      "[0. 1.]\n",
      "[0.01022197 0.989778  ]\n",
      "[0. 1.]\n",
      "[0.01040531 0.9895947 ]\n",
      "[0. 1.]\n",
      "[0.6645317 0.3354683]\n",
      "[1. 0.]\n",
      "[0.03570095 0.964299  ]\n",
      "[0. 1.]\n",
      "[0.37886378 0.62113625]\n",
      "[0. 1.]\n",
      "[0.5619771 0.4380229]\n",
      "[0. 1.]\n",
      "[0.00172665 0.9982734 ]\n",
      "[1. 0.]\n",
      "[0.6855037  0.31449628]\n",
      "[0. 1.]\n",
      "[0.10725798 0.8927421 ]\n",
      "[1. 0.]\n",
      "[4.9424812e-05 9.9995053e-01]\n",
      "[1. 0.]\n",
      "[0.0212229 0.9787771]\n",
      "[0. 1.]\n",
      "[1.17503325e-04 9.99882460e-01]\n",
      "[1. 0.]\n",
      "[0.00599726 0.99400276]\n",
      "[0. 1.]\n",
      "[0.7246216  0.27537844]\n",
      "[0. 1.]\n",
      "[0.02757368 0.9724263 ]\n",
      "[0. 1.]\n",
      "[0.01315664 0.9868434 ]\n",
      "[0. 1.]\n",
      "[2.4167364e-04 9.9975830e-01]\n",
      "[1. 0.]\n",
      "[0.8064449 0.1935551]\n",
      "[0. 1.]\n",
      "[4.7400830e-05 9.9995255e-01]\n",
      "[0. 1.]\n",
      "[0.3654433 0.6345568]\n",
      "[0. 1.]\n",
      "[0.00312958 0.99687046]\n",
      "[0. 1.]\n",
      "[0.47595206 0.524048  ]\n",
      "[0. 1.]\n",
      "[0.06637017 0.9336298 ]\n",
      "[0. 1.]\n",
      "[0.56784284 0.4321572 ]\n",
      "[0. 1.]\n",
      "[0.00834925 0.99165076]\n",
      "[1. 0.]\n",
      "[0.01813126 0.9818687 ]\n",
      "[1. 0.]\n",
      "[0.00361354 0.9963864 ]\n",
      "[0. 1.]\n",
      "[0.0053147 0.9946853]\n",
      "[1. 0.]\n",
      "[1.1281858e-06 9.9999893e-01]\n",
      "[0. 1.]\n",
      "[0.54733515 0.45266485]\n",
      "[0. 1.]\n",
      "[0.2974209 0.7025791]\n",
      "[0. 1.]\n",
      "[0.06281421 0.9371858 ]\n",
      "[0. 1.]\n",
      "[0.4152777  0.58472234]\n",
      "[0. 1.]\n",
      "[0.00168035 0.9983197 ]\n",
      "[1. 0.]\n",
      "[4.074921e-04 9.995925e-01]\n",
      "[0. 1.]\n",
      "[6.6739492e-05 9.9993324e-01]\n",
      "[0. 1.]\n",
      "[0.00652317 0.9934768 ]\n",
      "[1. 0.]\n",
      "[0.8650212  0.13497883]\n",
      "[0. 1.]\n",
      "[0.06404931 0.9359507 ]\n",
      "[1. 0.]\n",
      "[0.9678799  0.03212014]\n",
      "[0. 1.]\n",
      "[0.00459266 0.99540734]\n",
      "[0. 1.]\n",
      "[0.01466124 0.9853388 ]\n",
      "[0. 1.]\n",
      "[4.0529997e-04 9.9959475e-01]\n",
      "[0. 1.]\n",
      "[0.7022557 0.2977443]\n",
      "[1. 0.]\n",
      "[3.6801942e-04 9.9963200e-01]\n",
      "[1. 0.]\n",
      "[7.0946477e-04 9.9929047e-01]\n",
      "[0. 1.]\n",
      "[0.0176159 0.9823841]\n",
      "[0. 1.]\n",
      "[3.5083460e-05 9.9996495e-01]\n",
      "[0. 1.]\n",
      "[0.11257742 0.8874226 ]\n",
      "[0. 1.]\n",
      "[0.17147624 0.8285238 ]\n",
      "[0. 1.]\n",
      "[0.05121037 0.94878966]\n",
      "[0. 1.]\n",
      "[0.00274231 0.99725777]\n",
      "[0. 1.]\n",
      "[0.19250081 0.80749923]\n",
      "[0. 1.]\n",
      "[0.1410008 0.8589992]\n",
      "[1. 0.]\n",
      "[0.76672304 0.2332769 ]\n",
      "[0. 1.]\n",
      "[0.09686365 0.9031364 ]\n",
      "[1. 0.]\n",
      "[0.0051847 0.9948153]\n",
      "[0. 1.]\n",
      "[0.04980605 0.950194  ]\n",
      "[0. 1.]\n",
      "[0.4701731 0.5298269]\n",
      "[0. 1.]\n",
      "[5.5717700e-04 9.9944276e-01]\n",
      "[0. 1.]\n",
      "[0.00276808 0.9972319 ]\n",
      "[0. 1.]\n",
      "[8.0600916e-04 9.9919397e-01]\n",
      "[0. 1.]\n",
      "[0.43142393 0.5685761 ]\n",
      "[0. 1.]\n",
      "[0.2623007 0.7376993]\n",
      "[0. 1.]\n",
      "[1.1969925e-05 9.9998808e-01]\n",
      "[0. 1.]\n",
      "[7.988976e-05 9.999201e-01]\n",
      "[1. 0.]\n",
      "[2.2797385e-06 9.9999774e-01]\n",
      "[0. 1.]\n",
      "[8.008754e-05 9.999199e-01]\n",
      "[0. 1.]\n",
      "[0.23939453 0.76060545]\n",
      "[0. 1.]\n",
      "[0.0036513 0.9963486]\n",
      "[0. 1.]\n",
      "[0.9275417  0.07245832]\n",
      "[0. 1.]\n",
      "[0.1647744 0.8352256]\n",
      "[1. 0.]\n",
      "[0.3234805 0.6765195]\n",
      "[1. 0.]\n",
      "[9.6876716e-04 9.9903131e-01]\n",
      "[0. 1.]\n",
      "[0.02306226 0.9769378 ]\n",
      "[0. 1.]\n",
      "[0.0373598  0.96264017]\n",
      "[0. 1.]\n",
      "[0.06458541 0.93541455]\n",
      "[0. 1.]\n",
      "[0.00384748 0.99615246]\n",
      "[0. 1.]\n",
      "[6.035820e-06 9.999939e-01]\n",
      "[0. 1.]\n",
      "[0.7691827  0.23081726]\n",
      "[0. 1.]\n",
      "[0.01134951 0.98865044]\n",
      "[0. 1.]\n",
      "[0.188318   0.81168205]\n",
      "[1. 0.]\n",
      "[2.8660940e-04 9.9971336e-01]\n",
      "[1. 0.]\n",
      "[0.09671724 0.90328276]\n",
      "[1. 0.]\n",
      "[0.00178064 0.9982193 ]\n",
      "[1. 0.]\n",
      "[0.0192876 0.9807124]\n",
      "[1. 0.]\n",
      "[0.01683205 0.98316795]\n",
      "[1. 0.]\n",
      "[0.00168672 0.9983133 ]\n",
      "[0. 1.]\n",
      "[0.31126997 0.68873006]\n",
      "[0. 1.]\n",
      "[0.00102581 0.9989742 ]\n",
      "[0. 1.]\n",
      "[0.0974498  0.90255016]\n",
      "[1. 0.]\n",
      "[0.37073997 0.62926006]\n",
      "[0. 1.]\n",
      "[0.02304998 0.97695005]\n",
      "[1. 0.]\n",
      "[4.9625733e-06 9.9999499e-01]\n",
      "[1. 0.]\n",
      "[0.00293079 0.9970693 ]\n",
      "[0. 1.]\n",
      "[4.2037616e-04 9.9957961e-01]\n",
      "[0. 1.]\n",
      "[0.17739846 0.82260156]\n",
      "[0. 1.]\n",
      "[5.183672e-04 9.994816e-01]\n",
      "[0. 1.]\n",
      "[0.14834337 0.8516566 ]\n",
      "[0. 1.]\n",
      "[5.9617250e-06 9.9999404e-01]\n",
      "[0. 1.]\n",
      "[0.4055387 0.5944613]\n",
      "[0. 1.]\n",
      "[6.9652166e-04 9.9930346e-01]\n",
      "[0. 1.]\n",
      "[0.03537649 0.9646235 ]\n",
      "[0. 1.]\n",
      "[3.529606e-04 9.996470e-01]\n",
      "[0. 1.]\n",
      "[0.11570763 0.88429236]\n",
      "[0. 1.]\n",
      "[0.02665881 0.9733412 ]\n",
      "[1. 0.]\n",
      "[0.0020222 0.9979778]\n",
      "[0. 1.]\n",
      "[0.02031504 0.97968495]\n",
      "[0. 1.]\n",
      "[0.00295213 0.99704784]\n",
      "[0. 1.]\n",
      "[2.5468345e-05 9.9997449e-01]\n",
      "[0. 1.]\n",
      "[0.16878049 0.8312195 ]\n",
      "[1. 0.]\n",
      "[1.3846665e-05 9.9998617e-01]\n",
      "[0. 1.]\n",
      "[0.10230631 0.8976937 ]\n",
      "[0. 1.]\n",
      "[0.00483138 0.9951687 ]\n",
      "[0. 1.]\n",
      "[0.48622087 0.5137791 ]\n",
      "[0. 1.]\n",
      "[0.1673068 0.8326932]\n",
      "[0. 1.]\n",
      "[4.7205415e-07 9.9999952e-01]\n",
      "[0. 1.]\n",
      "[0.02783854 0.9721614 ]\n",
      "[0. 1.]\n",
      "[0.31539539 0.6846046 ]\n",
      "[1. 0.]\n",
      "[0.18522201 0.814778  ]\n",
      "[0. 1.]\n",
      "[1.5505937e-04 9.9984491e-01]\n",
      "[0. 1.]\n",
      "[1.9673073e-04 9.9980325e-01]\n",
      "[1. 0.]\n",
      "[0.0596822 0.9403178]\n",
      "[0. 1.]\n",
      "[0.82452136 0.17547856]\n",
      "[0. 1.]\n",
      "[0.91608524 0.08391479]\n",
      "[0. 1.]\n",
      "[0.00594439 0.9940556 ]\n",
      "[0. 1.]\n",
      "[0.04621689 0.9537831 ]\n",
      "[0. 1.]\n",
      "[0.26328343 0.7367165 ]\n",
      "[0. 1.]\n",
      "[2.2441336e-06 9.9999774e-01]\n",
      "[0. 1.]\n",
      "[0.19452141 0.8054786 ]\n",
      "[0. 1.]\n",
      "[0.00231661 0.9976834 ]\n",
      "[0. 1.]\n",
      "[8.6405640e-04 9.9913603e-01]\n",
      "[1. 0.]\n",
      "[8.476485e-04 9.991523e-01]\n",
      "[0. 1.]\n",
      "[0.32658625 0.6734138 ]\n",
      "[1. 0.]\n",
      "[0.0803662 0.9196338]\n",
      "[0. 1.]\n",
      "[0.03338939 0.96661055]\n",
      "[0. 1.]\n",
      "[0.0014485 0.9985514]\n",
      "[1. 0.]\n",
      "[5.9622456e-04 9.9940383e-01]\n",
      "[1. 0.]\n",
      "[0.12268915 0.8773109 ]\n",
      "[1. 0.]\n",
      "[0.02498142 0.9750185 ]\n",
      "[0. 1.]\n",
      "[0.12462123 0.8753788 ]\n",
      "[0. 1.]\n",
      "[3.5783980e-04 9.9964213e-01]\n",
      "[0. 1.]\n",
      "[7.5377837e-05 9.9992466e-01]\n",
      "[1. 0.]\n",
      "[0.00104177 0.99895823]\n",
      "[0. 1.]\n",
      "[2.0797785e-04 9.9979204e-01]\n",
      "[0. 1.]\n",
      "[0.23289974 0.7671003 ]\n",
      "[1. 0.]\n",
      "[0.11707189 0.8829281 ]\n",
      "[1. 0.]\n",
      "[4.915566e-06 9.999951e-01]\n",
      "[0. 1.]\n",
      "[0.5776794 0.4223206]\n",
      "[0. 1.]\n",
      "[0.07510254 0.92489743]\n",
      "[1. 0.]\n",
      "[0.9148004  0.08519959]\n",
      "[1. 0.]\n",
      "[9.7718660e-04 9.9902284e-01]\n",
      "[1. 0.]\n",
      "[1.4835694e-05 9.9998522e-01]\n",
      "[0. 1.]\n",
      "[0.00238959 0.99761045]\n",
      "[0. 1.]\n",
      "[0.84324956 0.15675047]\n",
      "[1. 0.]\n",
      "[0.06840731 0.93159264]\n",
      "[1. 0.]\n",
      "[1.7025428e-04 9.9982977e-01]\n",
      "[1. 0.]\n",
      "[3.0595012e-04 9.9969411e-01]\n",
      "[0. 1.]\n",
      "[0.11962673 0.8803733 ]\n",
      "[0. 1.]\n",
      "[0.00870841 0.9912916 ]\n",
      "[1. 0.]\n",
      "[0.00528371 0.99471635]\n",
      "[1. 0.]\n",
      "[0.00313499 0.996865  ]\n",
      "[0. 1.]\n",
      "[0.37377053 0.62622947]\n",
      "[0. 1.]\n",
      "[0.00108016 0.99891984]\n",
      "[1. 0.]\n",
      "[0.01483748 0.98516256]\n",
      "[0. 1.]\n",
      "[0.00556405 0.9944359 ]\n",
      "[0. 1.]\n",
      "[0.33551368 0.6644863 ]\n",
      "[1. 0.]\n",
      "[0.02071685 0.97928315]\n",
      "[1. 0.]\n",
      "[0.05322322 0.9467768 ]\n",
      "[1. 0.]\n",
      "[0.08064941 0.91935056]\n",
      "[0. 1.]\n",
      "[0.00651439 0.9934856 ]\n",
      "[0. 1.]\n",
      "[0.00178119 0.99821883]\n",
      "[0. 1.]\n",
      "[0.00115023 0.99884975]\n",
      "[0. 1.]\n",
      "[0.28828114 0.7117189 ]\n",
      "[0. 1.]\n",
      "[1.6633103e-05 9.9998331e-01]\n",
      "[0. 1.]\n",
      "[9.9670782e-04 9.9900335e-01]\n",
      "[0. 1.]\n",
      "[0.00385641 0.9961436 ]\n",
      "[0. 1.]\n",
      "[0.74464774 0.25535226]\n",
      "[0. 1.]\n",
      "[1.6812213e-05 9.9998319e-01]\n",
      "[0. 1.]\n",
      "[2.2555653e-06 9.9999774e-01]\n",
      "[0. 1.]\n",
      "[0.00775536 0.99224466]\n",
      "[1. 0.]\n",
      "[0.6425011  0.35749888]\n",
      "[1. 0.]\n",
      "[2.9438820e-07 9.9999976e-01]\n",
      "[0. 1.]\n",
      "[9.5592586e-05 9.9990439e-01]\n",
      "[1. 0.]\n",
      "[0.15092741 0.8490726 ]\n",
      "[1. 0.]\n",
      "[0.00672944 0.9932706 ]\n",
      "[0. 1.]\n",
      "[0.02496604 0.97503394]\n",
      "[0. 1.]\n",
      "[3.5514972e-06 9.9999642e-01]\n",
      "[0. 1.]\n",
      "[1.447852e-05 9.999856e-01]\n",
      "[0. 1.]\n",
      "[0.07126302 0.928737  ]\n",
      "[0. 1.]\n",
      "[0.12842223 0.87157774]\n",
      "[0. 1.]\n",
      "[0.01159274 0.9884072 ]\n",
      "[1. 0.]\n",
      "[0.05358096 0.94641906]\n",
      "[0. 1.]\n",
      "[0.00166015 0.9983398 ]\n",
      "[0. 1.]\n",
      "[0.18258215 0.81741786]\n",
      "[0. 1.]\n",
      "[0.09057434 0.9094257 ]\n",
      "[0. 1.]\n",
      "[0.29792032 0.70207965]\n",
      "[0. 1.]\n",
      "[0.6405268  0.35947317]\n",
      "[0. 1.]\n",
      "[0.42714602 0.572854  ]\n",
      "[1. 0.]\n",
      "[5.063130e-04 9.994937e-01]\n",
      "[0. 1.]\n",
      "[0.00940671 0.99059325]\n",
      "[0. 1.]\n",
      "[0.05327084 0.9467291 ]\n",
      "[0. 1.]\n",
      "[0.03788287 0.96211714]\n",
      "[0. 1.]\n",
      "[0.02851043 0.97148955]\n",
      "[1. 0.]\n",
      "[0.04959238 0.95040756]\n",
      "[1. 0.]\n",
      "[0.67932624 0.32067376]\n",
      "[0. 1.]\n",
      "[0.00649137 0.9935087 ]\n",
      "[1. 0.]\n",
      "[4.0380884e-04 9.9959618e-01]\n",
      "[1. 0.]\n",
      "[0.2902355 0.7097645]\n",
      "[0. 1.]\n",
      "[0.2618241  0.73817587]\n",
      "[1. 0.]\n",
      "[0.00577587 0.99422413]\n",
      "[0. 1.]\n",
      "[0.00321511 0.9967849 ]\n",
      "[0. 1.]\n",
      "[0.00216116 0.9978389 ]\n",
      "[1. 0.]\n",
      "[5.8306082e-06 9.9999416e-01]\n",
      "[0. 1.]\n",
      "[3.6054855e-06 9.9999642e-01]\n",
      "[1. 0.]\n",
      "[0.00135654 0.99864346]\n",
      "[0. 1.]\n",
      "[0.89219457 0.10780539]\n",
      "[1. 0.]\n",
      "[0.16362795 0.836372  ]\n",
      "[0. 1.]\n",
      "[0.07641359 0.92358637]\n",
      "[0. 1.]\n",
      "[0.05291995 0.94708   ]\n",
      "[0. 1.]\n",
      "[0.00115676 0.9988432 ]\n",
      "[0. 1.]\n",
      "[3.1936276e-04 9.9968064e-01]\n",
      "[0. 1.]\n",
      "[0.05329217 0.94670784]\n",
      "[0. 1.]\n",
      "[0.00580311 0.9941969 ]\n",
      "[1. 0.]\n",
      "[0.02128506 0.97871494]\n",
      "[0. 1.]\n",
      "[0.16572388 0.83427614]\n",
      "[0. 1.]\n",
      "[3.4768353e-04 9.9965227e-01]\n",
      "[1. 0.]\n",
      "[0.00204131 0.99795866]\n",
      "[0. 1.]\n",
      "[2.7639893e-04 9.9972361e-01]\n",
      "[0. 1.]\n",
      "[0.23828734 0.7617126 ]\n",
      "[0. 1.]\n",
      "[0.00173654 0.99826354]\n",
      "[1. 0.]\n",
      "[0.00923188 0.99076813]\n",
      "[0. 1.]\n",
      "[8.1497717e-05 9.9991846e-01]\n",
      "[0. 1.]\n",
      "[0.00786353 0.9921365 ]\n",
      "[0. 1.]\n",
      "[4.0141094e-05 9.9995983e-01]\n",
      "[0. 1.]\n",
      "[0.01172972 0.9882703 ]\n",
      "[0. 1.]\n",
      "[0.05419159 0.94580835]\n",
      "[0. 1.]\n",
      "[0.04853496 0.9514651 ]\n",
      "[1. 0.]\n",
      "[0.26028386 0.7397161 ]\n",
      "[1. 0.]\n",
      "[0.5841869 0.4158131]\n",
      "[0. 1.]\n",
      "[0.12549146 0.87450856]\n",
      "[0. 1.]\n",
      "[2.1823333e-04 9.9978179e-01]\n",
      "[0. 1.]\n",
      "[7.5618242e-04 9.9924386e-01]\n",
      "[0. 1.]\n",
      "[0.01529984 0.9847002 ]\n",
      "[0. 1.]\n",
      "[0.00554022 0.99445975]\n",
      "[0. 1.]\n",
      "[0.75209075 0.24790931]\n",
      "[0. 1.]\n",
      "[0.01706975 0.98293024]\n",
      "[0. 1.]\n",
      "[0.03396522 0.9660347 ]\n",
      "[0. 1.]\n",
      "[0.18563224 0.8143677 ]\n",
      "[0. 1.]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "#     sess.run(tf.local_variables_initializer())\n",
    "    ab = []\n",
    "    for i in range (len(test_x)):\n",
    "        xt = test_x[i][np.newaxis,:]\n",
    "        yt = test_y[i][np.newaxis,:]\n",
    "        predict_test = sess.run(predicted, feed_dict={inputs_: xt,labels_: yt})\n",
    "        ab.append(np.hstack((predict_test[0], yt[0])))\n",
    "#         print(np.hstack((predict_test[0], yt[0])))\n",
    "        print(np.array(predict_test[0]))\n",
    "        print(np.array(yt[0]))\n",
    "\n",
    "#     np.savetxt('new2.csv',ab,delimiter =',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 1 1]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HanY\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "data = ['北京', '上海', '广州', '成都', '杭州', '深圳']\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoded = label_encoder.fit_transform(labels)\n",
    "print(label_encoded)\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "one_hot_encoded = one_hot_encoder.fit_transform(label_encoded.reshape(-1, 1)).toarray()\n",
    "print(one_hot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,1,1,1])\n",
    " \n",
    "b = a[np.newaxis,:]\n",
    "c = a[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
